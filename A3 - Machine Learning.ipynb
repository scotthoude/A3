{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading all of the data files, all at 100mhz or 100 samples per second\n",
    "# Thanks to Katie for her files, I needed the sitting, jogging and stairs ones,\n",
    "# the others are my own\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "clsTrain = [] # this is store my array of classifiers (0-4) - kinda hacky but it works\n",
    "clsTest = []\n",
    "\n",
    "#This takes in the csv files and returns an array that can be used for machine learning\n",
    "#also returns the header row which is the same for all of my files\n",
    "def load_files(input_file, cls_num):\n",
    "    output_array = []\n",
    "    input_headers = []\n",
    "    with open(input_file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        input_headers = reader.next() #saves the headers\n",
    "        for row in reader:\n",
    "            output_array.append(row)\n",
    "            \n",
    "    # Prior testing gave me the size of the files so I know I can skip the first 10 seconds (1000 rows)\n",
    "    # and then take the next 30,000 records to get 30 - 10 second chunks\n",
    "    # never forget that it's all zero indexed\n",
    "    output_array = output_array[999:30999]\n",
    "    \n",
    "    # Now let's reduce down to 10 second chunks aka 1000 rows\n",
    "    compressed = []\n",
    "    n = 0\n",
    "    temp = []\n",
    "    \n",
    "    for row in output_array:\n",
    "        temp.append(row)\n",
    "        n +=1\n",
    "        if n == 1000: \n",
    "            temp = np.array(temp).astype(float) #convert to a numpy float array so I can do the next line\n",
    "            compressed.append(temp.mean(axis=0).tolist()) #this averages by columns and makes it a list of lists\n",
    "            temp = []\n",
    "            n = 0\n",
    "            \n",
    "    # I use this to build the 'y' array for use later\n",
    "    for i in range(30):\n",
    "        if i < 24:\n",
    "            clsTrain.append(cls_num)\n",
    "        else:\n",
    "            clsTest.append(cls_num)\n",
    "            \n",
    "#     compressed = list(itertools.chain.from_iterable(compressed)) # this flatens the list - which turns to not be wanted\n",
    "\n",
    "    return compressed, input_headers\n",
    "    \n",
    "rawWalkValues, Labels = load_files(\"walking2.csv\",0)\n",
    "rawSittingValues, Labels = load_files(\"sitting.csv\",1)\n",
    "rawCarValues, Labels = load_files(\"car.csv\",2)\n",
    "rawJogValues, Labels = load_files(\"Jogging.csv\",3)\n",
    "rawStairsValues, Labels = load_files(\"Steps.csv\",4)\n",
    "\n",
    "# put everything together into one merged training and then seperate test sets\n",
    "# first 24 records from each  train, last 6 = testing\n",
    "allTrain = rawWalkValues[:24] + rawSittingValues[:24] + rawCarValues[:24] + rawJogValues[:24] + rawStairsValues[:24]\n",
    "walkTest = rawWalkValues[24:] \n",
    "sitTest = rawSittingValues[24:]\n",
    "carTest = rawCarValues[24:]\n",
    "jogTest = rawJogValues[24:]\n",
    "stairsTest = rawStairsValues[24:]\n",
    "# allTest = rawWalkValues[24:] + rawSittingValues[24:] + rawCarValues[24:] + rawJogValues[24:] + rawStairsValues[24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This block is just to test my data loads and various setup stuff from the block above\n",
    "# print Labels\n",
    "# print rawWalkValues[0]\n",
    "# print clsTrain\n",
    "# print len(rawWalkValues[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing the length of the arrays, I should have 30 records after chunking into 10 second sections by / 1000\n",
    "# print \"walk -\",len(rawWalkValues)\n",
    "# print \"sit -\",len(rawSittingValues)\n",
    "# print \"car -\",len(rawCarValues)\n",
    "# print \"jog -\",len(rawJogValues)\n",
    "# print \"stairs -\",len(rawStairsValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trying some feature selection, not a fan of this method, see comments below\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8*(1-.8)))\n",
    "# print sel.fit_transform(rawWalkValues)[0]\n",
    "\n",
    "# So this strips out the low variance stuff but doesn't help me identify the right features, just removes certain columns\n",
    "# I can do (and did) a manual compare but that just seems wrong for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# another attempt at feature selection - which doesn't work for negative values, boo\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "# iris = load_iris()\n",
    "# X, y = rawWalkValues, rawLabels\n",
    "\n",
    "# X = np.array(X)\n",
    "# X.shape\n",
    "\n",
    "# X_new = SelectKBest(chi2, k=2).fit_transform(X, rawLabels)\n",
    "# X_new.shape\n",
    "\n",
    "# This doesn't work for negative values which is what I have - so this selection is out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False  True False\n",
      "  True False False False]\n",
      "[15  5  8 11  9 10  7  4  6  2  1  3  1 14 13 12]\n",
      "Two Most Important Features are:\n",
      "   --  user_acc_x\n",
      "   --  user_acc_z\n",
      "The Most Important Feature is:\n",
      "   --  user_acc_x\n"
     ]
    }
   ],
   "source": [
    "# Yet another attempt at feature selection\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html for details\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator,2,step=1) # select the n most informative features\n",
    "selector = selector.fit(allTrain, clsTrain)\n",
    "\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "print \"Two Most Important Features are:\"\n",
    "rank = selector.ranking_\n",
    "for i in range(len(rank)):\n",
    "    if rank[i] == 1:\n",
    "        print \"   -- \",Labels[i]\n",
    "        \n",
    "\n",
    "selector = RFE(estimator,1,step=1) # select the n most informative features\n",
    "selector = selector.fit(allTrain, clsTrain)\n",
    "print \"The Most Important Feature is:\"\n",
    "rank = selector.ranking_\n",
    "for i in range(len(rank)):\n",
    "    if rank[i] == 1:\n",
    "        print \"   -- \",Labels[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_acc_x\n",
      "user_acc_z\n"
     ]
    }
   ],
   "source": [
    "rank = selector.ranking_\n",
    "\n",
    "for i in range(len(rank)):\n",
    "    if rank[i] == 1:\n",
    "        print Labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-252-94f9bb046cc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#create an index array, with the number of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mfeatures_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimportances\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#only keep features whose importance is greater than the mean importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;31m#should be about an array of size 3 (about)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mfeatures_to_keep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 14 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "#Let's do some feature selection - which isn't the same as extraction\n",
    "#http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=10,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False)\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "# forest.fit(X, y)\n",
    "forest.fit(allTrain, clsTrain)\n",
    "\n",
    "importances = forest.feature_importances_ #array with importances of each feature\n",
    "\n",
    "idx = np.arange(0, X.shape[1]) #create an index array, with the number of features\n",
    "\n",
    "features_to_keep = idx[importances > np.mean(importances)] #only keep features whose importance is greater than the mean importance\n",
    "#should be about an array of size 3 (about)\n",
    "print features_to_keep.shape\n",
    "\n",
    "x_feature_selected = X[:,features_to_keep] #pull X values corresponding to the most important features\n",
    "\n",
    "# print x_feature_selected\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn the raw data into a more compressed format and split into training and testing arrays\n",
    "# based on all my files being a little longer than needed I'm going to drop the first 10 seconds (1000 rows)\n",
    "# # take the next 24 (10 second chunks or 24000 rows) as training, and then the next 6 (6000 rows) as testing\n",
    "# compressed = []\n",
    "\n",
    "# n = 0\n",
    "# temp = 0\n",
    "\n",
    "# for row in rawWalkValues:\n",
    "#     temp += float(row[7]) # TODO:  What column should I use? and would it be different for different files\n",
    "#     n +=1\n",
    "#     if n == 1000:\n",
    "#         compressed.append(temp/n)\n",
    "#         temp = 0\n",
    "#         n = 0\n",
    "    \n",
    "# #splitting it up into two sets\n",
    "# train = []\n",
    "# test = []\n",
    "\n",
    "# i = 0\n",
    "# for i in range(len(compressed)):\n",
    "#     if i < 24:\n",
    "#         train.append(compressed[1])\n",
    "#     elif i < 30:\n",
    "#         test.append(compressed[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where the Decision Tree Learning Happens\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "  \n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(allTrain, clsTrain)\n",
    "\n",
    "clf.predict(stairsTest)\n",
    "# clf.predict_proba(walkTest)\n",
    "# clf.predict_proba(sitTest)\n",
    "# clf.predict_proba(carTest)\n",
    "# clf.predict_proba(jogTest)\n",
    "# clf.predict_proba(stairsTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

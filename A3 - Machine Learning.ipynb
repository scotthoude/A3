{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my handin for CSCI-2951-R Assignment 3\n",
    "\n",
    "I've tried to leave informative comments throughout and cited any sklearn pages where I might have gotten some code snippets.  I've also placed some markdown cells throughout to help describe my train of thought.  However to summarize.  I loaded my datafiles, merged them and shaped them to be passed to machine learning algorithms.  To begin I used all 15 fields and took the mean only of 1000 records to make 1 value for each 10 second segment of time.  I did a multi-classifier and compared all five files at the same time.  All three methods I tried were very successful.\n",
    "\n",
    "Feeling like using all 15 fields was cheating I implemented a feature selection algorithm to find the most informative features, and then tested on just those fields.  Those features didn't get me 100% accuracy.  So implemented another feature selection which highlighted some different fields that turned out to be highly accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading all of the data files, all at 100mhz or 100 samples per second\n",
    "# Thanks to Katie for her files, I needed the sitting, jogging and stairs ones,\n",
    "# the others are my own\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "# these magic rows make the long floats in a np array all output nicely for display\n",
    "float_formatter = lambda x: \"%.3f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "clsTrain = [] # this is store my array of classifiers (0-4) - kinda hacky but it works\n",
    "clsTest = []\n",
    "\n",
    "#This takes in the csv files and returns an array that can be used for machine learning\n",
    "#also returns the header row which is the same for all of my files\n",
    "def load_files(input_file, cls_num):\n",
    "    output_array = []\n",
    "    input_headers = []\n",
    "    with open(input_file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        input_headers = reader.next() #saves the headers\n",
    "        for row in reader:\n",
    "            output_array.append(row[1:]) # Don't need the timestamp\n",
    "    \n",
    "    input_headers = input_headers[1:] # again - don't need the timestamp\n",
    "    # Convert to a dict to make using it easier later on\n",
    "    output_headers = {}\n",
    "    for i in range(len(input_headers)):\n",
    "        output_headers[i] = input_headers[i]\n",
    "            \n",
    "    # Prior testing gave me the size of the files so I know I can skip the first 10 seconds (1000 rows)\n",
    "    # and then take the next 30,000 records to get 30 - 10 second chunks\n",
    "    # never forget that it's all zero indexed\n",
    "    output_array = output_array[999:30999]\n",
    "    \n",
    "    # Now let's reduce down to 10 second chunks aka 1000 rows\n",
    "    compressed = []\n",
    "    n = 0\n",
    "    temp = []\n",
    "    \n",
    "    for row in output_array:\n",
    "        temp.append(row)\n",
    "        n +=1\n",
    "        if n == 1000: \n",
    "            temp = np.array(temp).astype(float) #convert to a numpy float array so I can do the next line\n",
    "            compressed.append(temp.mean(axis=0).tolist()) #this averages by columns and makes it a list of lists\n",
    "            temp = []\n",
    "            n = 0\n",
    "            \n",
    "    # I use this to build the 'y' or classifier array for use later\n",
    "    for i in range(30):\n",
    "        if i < 24:\n",
    "            clsTrain.append(cls_num)\n",
    "        else:\n",
    "            clsTest.append(cls_num)\n",
    "            \n",
    "\n",
    "    return compressed, output_headers\n",
    "    \n",
    "# rawWalkValues, Labels = load_files(\"walking2.csv\",0) # my walking - which doesn't play well with Katie's movement data\n",
    "rawWalkValues, Labels = load_files(\"Walking-katie.csv\",0) # Katie's walking\n",
    "rawSittingValues, Labels = load_files(\"sitting.csv\",1)\n",
    "rawCarValues, Labels = load_files(\"car.csv\",2)\n",
    "rawJogValues, Labels = load_files(\"Jogging.csv\",3)\n",
    "rawStairsValues, Labels = load_files(\"Steps.csv\",4)\n",
    "\n",
    "# This dictionary helps translate from an int to the type of activity downstream\n",
    "dataType = {0:'Walking', 1:'Sitting', 2:'Car', 3:'Jogging', 4:'Stairs'}\n",
    "\n",
    "# put everything together into one merged training and then seperate test sets\n",
    "# first 24 records from each  train, last 6 = testing\n",
    "allTrain = rawWalkValues[:24] + rawSittingValues[:24] + rawCarValues[:24] + rawJogValues[:24] + rawStairsValues[:24]\n",
    "walkTest = rawWalkValues[24:] \n",
    "sitTest = rawSittingValues[24:]\n",
    "carTest = rawCarValues[24:]\n",
    "jogTest = rawJogValues[24:]\n",
    "stairsTest = rawStairsValues[24:]\n",
    "allTest = rawWalkValues[24:] + rawSittingValues[24:] + rawCarValues[24:] + rawJogValues[24:] + rawStairsValues[24:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This block is just to test my data loads and various setup stuff from the block above\n",
    "# Feel free to use it to see anything in particular\n",
    "# print Labels\n",
    "# print rawWalkValues[0]\n",
    "# print clsTrain\n",
    "# print len(rawWalkValues[:24])\n",
    "# print dataType[0]\n",
    "\n",
    "#Testing the length of the arrays, I should have 30 records after chunking into 10 second sections by / 1000\n",
    "# print \"walk -\",len(rawWalkValues)\n",
    "# print \"sit -\",len(rawSittingValues)\n",
    "# print \"car -\",len(rawCarValues)\n",
    "# print \"jog -\",len(rawJogValues)\n",
    "# print \"stairs -\",len(rawStairsValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So now that I've got my data loaded and in the correct format lets try some machine learning.\n",
    "At this point I'm doing a multi-classifier with all five data sets combined and using all 15 data fields, more on that later\n",
    "\n",
    "Since all of my data was 100MHZ and I had a 1000 records per 10 second chunk I only used the means of each field. I felt that with that much data it would probably work pretty good, which it does.  Instead of changing the aggregation calculation up I did some interesting ML things later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where the Decision Tree Learning Happens\n",
    "# This is a multi-classifier for all five conditions \n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "  \n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(allTrain, clsTrain)\n",
    "\n",
    "# Full disclosure - if you run this a few times on my walking data (not Katie's) you could get different output results each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for all test data: 0.933333333333\n",
      "Column Data Types\n",
      "['Walking', 'Sitting', 'Car', 'Jogging', 'Stairs']\n",
      "Walking: Score for walk test  1.0 \n",
      "[[1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]]\n",
      "Sitting: Score for sit test  1.0 \n",
      "[[0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]]\n",
      "Car drive: Score for car test  1.0 \n",
      "[[0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]]\n",
      "Jogging: Score for jog test  0.666666666667 \n",
      "[[0.000 0.000 0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]]\n",
      "Stairs: Score for stairs test  1.0 \n",
      "[[0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]]\n"
     ]
    }
   ],
   "source": [
    "# the results for the Decision Tree learning\n",
    "print \"Score for all test data:\",clf.score(allTest, clsTest)\n",
    "# In multi-label classification, this is the subset accuracy which is a harsh metric since you require for \n",
    "# each sample that each label set be correctly predicted.\n",
    "\n",
    "# print \"Score for Sit test data:\",clf.score(sitTest, [1,1,1,1,1,1])\n",
    "\n",
    "# clf.predict(walkTest) # the walkTest is interesting because it sometimes gets sitting, but mostly finds walking\n",
    "clfwalkResults = clf.predict_proba(walkTest)\n",
    "clfsitResults = clf.predict_proba(sitTest)\n",
    "clfcarResults =  clf.predict_proba(carTest)\n",
    "clfjogResults =  clf.predict_proba(jogTest)\n",
    "clfstairsResults =  clf.predict_proba(stairsTest)\n",
    "\n",
    "print \"Column Data Types\"\n",
    "print dataType.values()\n",
    "print \"Walking: Score for walk test \",clf.score(walkTest, [0,0,0,0,0,0]),\"\\n\", clfwalkResults\n",
    "print \"Sitting: Score for sit test \",clf.score(sitTest, [1,1,1,1,1,1]),\"\\n\", clfsitResults\n",
    "print \"Car drive: Score for car test \",clf.score(carTest, [2,2,2,2,2,2]),\"\\n\", clfcarResults\n",
    "print \"Jogging: Score for jog test \",clf.score(jogTest, [3,3,3,3,3,3]),\"\\n\", clfjogResults\n",
    "print \"Stairs: Score for stairs test \",clf.score(stairsTest, [4,4,4,4,4,4]),\"\\n\", clfstairsResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, penalty='l2',\n",
       "          random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression ML\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "logistic.fit(allTrain, clsTrain)\n",
    "# LogisticRegression(C=100000.0, class_weight=None, dual=False,fit_intercept=True, intercept_scaling=1,\n",
    "#                     penalty='l2', random_state=None, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for all test data: 1.0\n",
      "Column Data Types\n",
      "['Walking', 'Sitting', 'Car', 'Jogging', 'Stairs']\n",
      "Walking: Score for walk test  1.0 \n",
      "[[1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]\n",
      " [1.000 0.000 0.000 0.000 0.000]]\n",
      "Sitting: Score for sit test  1.0 \n",
      "[[0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]\n",
      " [0.000 1.000 0.000 0.000 0.000]]\n",
      "Car drive: Score for car test  1.0 \n",
      "[[0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.000 0.000 1.000 0.000 0.000]\n",
      " [0.012 0.000 0.982 0.000 0.005]]\n",
      "Jogging: Score for jog test  1.0 \n",
      "[[0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]\n",
      " [0.000 0.000 0.000 1.000 0.000]]\n",
      "Stairs: Score for stairs test  1.0 \n",
      "[[0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]]\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression output\n",
    "\n",
    "# logistic.predict(walkTest)\n",
    "# logistic.predict(sitTest)\n",
    "# logistic.predict(carTest)\n",
    "# logistic.predict(jogTest)\n",
    "# logistic.predict(stairsTest)\n",
    "# logistic.predict_proba(stairsTest)\n",
    "\n",
    "logwalkResults = logistic.predict_proba(walkTest)\n",
    "logsitResults = logistic.predict_proba(sitTest)\n",
    "logcarResults =  logistic.predict_proba(carTest)\n",
    "logjogResults =  logistic.predict_proba(jogTest)\n",
    "logstairsResults =  logistic.predict_proba(stairsTest)\n",
    "\n",
    "print \"Score for all test data:\",logistic.score(allTest, clsTest)\n",
    "# In multi-label classification, this is the subset accuracy which is a harsh metric since you require for \n",
    "# each sample that each label set be correctly predicted.\n",
    "\n",
    "print \"Column Data Types\"\n",
    "print dataType.values()\n",
    "print \"Walking: Score for walk test \",logistic.score(walkTest, [0,0,0,0,0,0]),\"\\n\", logwalkResults\n",
    "print \"Sitting: Score for sit test \",logistic.score(sitTest, [1,1,1,1,1,1]),\"\\n\", logsitResults\n",
    "print \"Car drive: Score for car test \",logistic.score(carTest, [2,2,2,2,2,2]),\"\\n\", logcarResults\n",
    "print \"Jogging: Score for jog test \",logistic.score(jogTest, [3,3,3,3,3,3]),\"\\n\", logjogResults\n",
    "print \"Stairs: Score for stairs test \",logistic.score(stairsTest, [4,4,4,4,4,4]),\"\\n\", logstairsResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Machine Learning with Support Vector Machines\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(allTrain, clsTrain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for all test data: 1.0\n",
      "Walking: Score for walk test  1.0 :Results  [0 0 0 0 0 0]\n",
      "Sitting: Score for sit test  1.0 :Results  [1 1 1 1 1 1]\n",
      "Car drive: Score for car test  1.0 :Results  [2 2 2 2 2 2]\n",
      "Jogging: Score for jog test  1.0 :Results  [3 3 3 3 3 3]\n",
      "Stairs: Score for stairs test  1.0 :Results  [4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "# Machine learning Output\n",
    "\n",
    "svcwalkResults = svc.predict(walkTest)\n",
    "svcsitResults = svc.predict(sitTest)\n",
    "svccarResults =  svc.predict(carTest)\n",
    "svcjogResults =  svc.predict(jogTest)\n",
    "svcstairsResults =  svc.predict(stairsTest)\n",
    "\n",
    "print \"Score for all test data:\",svc.score(allTest, clsTest)\n",
    "# In multi-label classification, this is the subset accuracy which is a harsh metric since you require for \n",
    "# each sample that each label set be correctly predicted.\n",
    "\n",
    "# print \"Column Data Types\"\n",
    "# print dataType.values()\n",
    "print \"Walking: Score for walk test \",svc.score(walkTest, [0,0,0,0,0,0]),\":Results \", svcwalkResults\n",
    "print \"Sitting: Score for sit test \",svc.score(sitTest, [1,1,1,1,1,1]),\":Results \", svcsitResults\n",
    "print \"Car drive: Score for car test \",svc.score(carTest, [2,2,2,2,2,2]),\":Results \", svccarResults\n",
    "print \"Jogging: Score for jog test \",svc.score(jogTest, [3,3,3,3,3,3]),\":Results \", svcjogResults\n",
    "print \"Stairs: Score for stairs test \",svc.score(stairsTest, [4,4,4,4,4,4]),\":Results \", svcstairsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see my machine learning works pretty good, but at this point I'm just feeding the algorithms all 15 fields from my data set and taking the mean.  I wonder how it would change if I instead just used the most relevant fields.\n",
    "\n",
    "To find those fields I can do one of these:<br>\n",
    "1) Manually try every field in the set one at a time (or groups of N, ugh)<br>\n",
    "2) Visually plot the information and look for what fields I should use<br>\n",
    "3) Use Feature Selection, aka machine learning to find the most informative fields in the data<br>\n",
    "\n",
    "1 would work but just plain feels wrong given better methods.  I did do some of 2 and mapping the lines suggested using the accelerometer data, the x and z directions mostly.\n",
    "\n",
    "However I really wanted to put machine learning to work and do feature selection. So I started working my way through this page\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html and began working my way down the methods on that page.\n",
    "\n",
    "The next few blocks cover what I found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The variance threshold eliminates fields (columns) of data what have a low variance, meaning they aren't informative\n",
    "# after trying I'm not a fan of this method, see comments below\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8*(1-.8)))\n",
    "# print sel.fit_transform(allTrain)[0] # uncomment this to see results\n",
    "\n",
    "# So this strips out the low variance stuff but doesn't help me identify the right features, just removes certain features\n",
    "# I can do (and did) a manual compare but that just seems wrong for some reason, this ends up reducing down to just 9 columns\n",
    "# out of the original 15 - adjusting the variance threshold brings this up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chi squared feature selection - which doesn't work for negative values, boo\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# X_new = SelectKBest(chi2, k=2).fit_transform(allTrain, clsTrain)\n",
    "# X_new.shape\n",
    "\n",
    "# This doesn't work for negative values which is what I have - so this selection is out\n",
    "# I could have normalized or shifted everything to be positive but other methods worked so I didn't bother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attitude_roll', 'attitude_pitch', 'attitude_yaw', 'rotation_rate_x', 'rotation_rate_y', 'rotation_rate_z', 'gravity_x', 'gravity_y', 'gravity_z', 'user_acc_x', 'user_acc_y', 'user_acc_z', 'magnetic_field_x', 'magnetic_field_y', 'magnetic_field_z']\n",
      "[ True False False False False  True False False False False False False\n",
      " False False False]\n",
      "[ 1  6  2 10  7  1 12  9  4  3  8  5 14 13 11]\n",
      "Two Most Important Features are:\n",
      "   --  attitude_roll\n",
      "   --  rotation_rate_z\n",
      "The Most Important Feature is:\n",
      "   --  rotation_rate_z\n"
     ]
    }
   ],
   "source": [
    "# This attempt uses recursive feature elimination to find the n most informative features\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html for details\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator,2,step=1) # select the n most informative features\n",
    "selector = selector.fit(allTrain, clsTrain) # the magic\n",
    "\n",
    "print Labels.values()\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "print \"Two Most Important Features are:\"\n",
    "rank = selector.ranking_\n",
    "for i in range(len(rank)):\n",
    "    if rank[i] == 1:\n",
    "        print \"   -- \",Labels[i]\n",
    "        \n",
    "selector = RFE(estimator,1,step=1) # select the n most informative features\n",
    "selector = selector.fit(allTrain, clsTrain)\n",
    "print \"The Most Important Feature is:\"\n",
    "rank = selector.ranking_\n",
    "for i in range(len(rank)):\n",
    "    if rank[i] == 1:\n",
    "        print \"   -- \",Labels[i]\n",
    "        \n",
    "# A frustrating thing is that this seems to provide different answers each time you run it also\n",
    "# when using my walk data it said acc_X and acc_z, now it's saying roll and rotation ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first ran this the feature selection method matched what I expected to see, user_acc_x and user_acc_z.  However now it's telling me two entirely different fields. I tested on the top 2 and 3 fields but I have my prior results saved in the text block below also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reducing traing down to just acc_x and acc_z\n",
    "\n",
    "def reduceToXZ(input_array):\n",
    "    output_array = []\n",
    "    for i in range(len(input_array)):\n",
    "        temp = []\n",
    "        temp.append(input_array[i][0]) # \n",
    "#         temp.append(input_array[i][2]) # Adding this one back in makes it much better\n",
    "        temp.append(input_array[i][5]) # \n",
    "        output_array.append(temp)\n",
    "    \n",
    "    return output_array\n",
    "    \n",
    "trainAxAz = reduceToXZ(allTrain)\n",
    "testAxAz = reduceToXZ(allTest)\n",
    "walkAxAz = reduceToXZ(walkTest)\n",
    "sitAxAz = reduceToXZ(sitTest)\n",
    "carAxAz = reduceToXZ(carTest)\n",
    "jogAxAz = reduceToXZ(jogTest)\n",
    "stairsAxAz = reduceToXZ(stairsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for all test data: 0.5\n",
      "Walking: Score for walk test  0.0 :Results  [3 3 3 3 3 2]\n",
      "Sitting: Score for sit test  1.0 :Results  [1 1 1 1 1 1]\n",
      "Car drive: Score for car test  1.0 :Results  [2 2 2 2 2 2]\n",
      "Jogging: Score for jog test  0.166666666667 :Results  [1 1 1 1 1 3]\n",
      "Stairs: Score for stairs test  0.333333333333 :Results  [2 4 2 4 2 2]\n"
     ]
    }
   ],
   "source": [
    "#SVM on reduced elements\n",
    "\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(trainAxAz, clsTrain)  \n",
    "\n",
    "# Machine learning Output on the reduced data set\n",
    "\n",
    "svcwalkResults = svc.predict(walkAxAz)\n",
    "svcsitResults = svc.predict(sitAxAz)\n",
    "svccarResults =  svc.predict(carAxAz)\n",
    "svcjogResults =  svc.predict(jogAxAz)\n",
    "svcstairsResults =  svc.predict(stairsAxAz)\n",
    "\n",
    "print \"Score for all test data:\",svc.score(testAxAz, clsTest)\n",
    "# In multi-label classification, this is the subset accuracy which is a harsh metric since you require for \n",
    "# each sample that each label set be correctly predicted.\n",
    "\n",
    "# print \"Column Data Types\"\n",
    "# print dataType.values()\n",
    "print \"Walking: Score for walk test \",svc.score(walkAxAz, [0,0,0,0,0,0]),\":Results \", svcwalkResults\n",
    "print \"Sitting: Score for sit test \",svc.score(sitAxAz, [1,1,1,1,1,1]),\":Results \", svcsitResults\n",
    "print \"Car drive: Score for car test \",svc.score(carAxAz, [2,2,2,2,2,2]),\":Results \", svccarResults\n",
    "print \"Jogging: Score for jog test \",svc.score(jogAxAz, [3,3,3,3,3,3]),\":Results \", svcjogResults\n",
    "print \"Stairs: Score for stairs test \",svc.score(stairsAxAz, [4,4,4,4,4,4]),\":Results \", svcstairsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, just the means of those two features did no better than a coin flip.  The above data is for the currently suggested two highest fields, it does get up to 93% if you add the third field in.\n",
    "\n",
    "Here are my results using just acc_x and acc_z (which had previously been suggested)\n",
    "Score for all test data: 0.3 <br> \n",
    "Walking: Score for walk test  0.0 :Results  [1 2 2 2 2 2] <br> \n",
    "Sitting: Score for sit test  1.0 :Results  [1 1 1 1 1 1] <br> \n",
    "Car drive: Score for car test  0.0 :Results  [1 1 1 1 1 1] <br> \n",
    "Jogging: Score for jog test  0.5 :Results  [4 3 4 3 1 3]<br> \n",
    "Stairs: Score for stairs test  0.0 :Results  [1 1 1 1 1 1]<br> \n",
    "\n",
    "When I added the next highest rank field (gravity_z) it got a little better, but adding the next few didn't help\n",
    "Score for all test data: 0.766666666667<br> \n",
    "Walking: Score for walk test  0.0 :Results  [4 4 4 4 4 4]<br> \n",
    "Sitting: Score for sit test  1.0 :Results  [1 1 1 1 1 1]<br> \n",
    "Car drive: Score for car test  0.833333333333 :Results  [2 2 2 2 2 1]<br> \n",
    "Jogging: Score for jog test  1.0 :Results  [3 3 3 3 3 3]<br> \n",
    "Stairs: Score for stairs test  1.0 :Results  [4 4 4 4 4 4]<br> \n",
    "\n",
    "So maybe my RFE wasn't the best way to look for features.  So I then tried something else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 6 :gravity_x (0.134055)\n",
      "2. feature 1 :attitude_pitch (0.133301)\n",
      "3. feature 13 :magnetic_field_y (0.119380)\n",
      "4. feature 7 :gravity_y (0.115556)\n",
      "5. feature 8 :gravity_z (0.093274)\n",
      "6. feature 0 :attitude_roll (0.074118)\n",
      "7. feature 12 :magnetic_field_x (0.058409)\n",
      "8. feature 5 :rotation_rate_z (0.057465)\n",
      "9. feature 2 :attitude_yaw (0.054389)\n",
      "10. feature 14 :magnetic_field_z (0.051906)\n",
      "11. feature 4 :rotation_rate_y (0.027585)\n",
      "12. feature 9 :user_acc_x (0.027121)\n",
      "13. feature 3 :rotation_rate_x (0.022556)\n",
      "14. feature 10 :user_acc_y (0.017865)\n",
      "15. feature 11 :user_acc_z (0.013021)\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAyAAAAJTCAYAAADwsXKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xl0VeW9//HPDomZCbMkTLGBlNAoGQCjTFEmSbVF5kCw\n",
       "KATvsle9gV8ZXAJSUWyaK5ROF7BEAm0EY6mVxQWJBZlSwAzixeTSVgkKQVQIhJCEaf/+yM1ZHE8S\n",
       "Mu6c5Lxfa53l9dnPs5/vPie95pPz7GcbpmmaAgAAAAALuLV0AQAAAABcBwEEAAAAgGUIIAAAAAAs\n",
       "QwABAAAAYBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAA\n",
       "ACxDAAEAAABgGQIIAAAAAMsQQAAAAABYhgACAAAAwDIEEAAAAACWIYAAAAAAsAwBBAAAAIBlCCAA\n",
       "AAAALEMAAQAAAGAZAggAAAAAyxBAAAAAAFiGAAIAAADAMgQQAAAAAJYhgAAAAACwDAEEAAAAgGUI\n",
       "IAAAAAAsQwABAAAAYBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACA\n",
       "ZQggAAAAACxDAAEAAABgGQIIAAAAAMsQQAAADmJjY+Xmxn8iAABNj/+6AGhT3Nzcan1t2rTJ0loe\n",
       "eughy+ZrSoZhyDCMli6j2e3bt09ubm5asWJFS5cCAC7DvaULAICmZhiGli9fXu2xyMhIy2tpjdLS\n",
       "0lRWVtbSZVimtX5OANAaEUAAtEnLli1r6RJatV69erV0CZYyTbOlSwAAl8ESLAAu68KFC1qyZInC\n",
       "wsLk4+OjDh06aPTo0dqzZ49D38uXL+uXv/ylHn74YfXs2VOenp7q1q2bfvzjH+vvf/+7Xd8333zT\n",
       "dv9E1RKfqlfVUp87Lf0JDg7WPffcU+15N23apF27dik2NlYBAQF292rcuHFDv/vd7xQTE6P27dvL\n",
       "19dXUVFR+u1vf1uvX7Kruwfk9po/+ugjPfLII+rQoYM6duyoSZMm6YsvvpAk/etf/9K0adPUtWtX\n",
       "+fj46KGHHtLx48cd5pg9e7bc3Nz0+eef6/XXX1f//v3l7e2tXr16af78+SopKam2tuzsbE2aNEnd\n",
       "unWTl5eXgoOD9dOf/lTnzp2rdY5f//rXuu+++2w1Pfnkk3r44YclSStWrLD7nPbv3y+pfp97laql\n",
       "d99++63mzZunwMBAeXl5KTw8XG+++WaN7/n777+vxx57zHZdvXv31oQJE/TBBx849N29e7fi4uLU\n",
       "pUsXeXl5qW/fvlq4cKEuXbrk0Pf48eOKj49XcHCwvLy81K1bN0VHRyspKUk3btyosR4AaC58AwLA\n",
       "JRUWFio2NlaFhYUaMWKE4uLidOXKFe3YsUOPPPKI1q1bp7lz59r6f/rpp3rxxRc1cuRIPfbYY+rY\n",
       "saMKCwv117/+Vf/93/+t9957T+PGjZNUucxr+fLlWrFihYKDgzV79mzbeWJjY+3qqG3pT03HMjIy\n",
       "tGvXLsXFxemZZ55RYWGhJOn69et67LHH9P7776t///5KSEiQl5eX/va3v+nZZ5/VkSNHlJaWVuf3\n",
       "qKb5jx07pl/84heKjY3VvHnzdPz4cW3fvl2ffPKJtm/frhEjRugHP/iBZs+erVOnTunPf/6zxowZ\n",
       "o88++0y+vr4O5/uP//gPHThwQNOmTVOHDh20a9curVmzRgcOHNDBgwfl6elp67tjxw5NmjRJhmFo\n",
       "8uTJ6tOnjz766CP9/ve/17vvvquDBw8qODjYYY7nn39eBw4c0KOPPqpHH31U7dq106BBgyRJmzZt\n",
       "UmxsrN1nU3WO+nzutysuLtbQoUPl6empqVOnqqKiQtu2bdNTTz0lNzc3PfHEE3b9ly9frpdffln+\n",
       "/v6aMGGCevXqpTNnzujw4cP64x//qFGjRtn6rlixQitWrFDnzp1tgeXjjz9WSkqKdu7cqaysLPn7\n",
       "+0uqDB/333+/2rVrpx/96Ee65557dPnyZf3jH//Q73//e73yyityd+dXAQAWMwGgDTEMwzQMw3zp\n",
       "pZfM5cuX273efPNNW7+RI0ea7dq1M7du3Wo3vri42IyIiDC9vb3Nr776ytZ+6dIl89tvv3WY78sv\n",
       "vzSDgoLMsLCwamt56KGHqq1z7969pmEY5ooVK6o93qdPH/Oee+6xa0tNTTUNwzDbtWtn7t6922HM\n",
       "8uXLTcMwzOeee868deuWrf3mzZvmnDlzTMMwzHfffbfa+b5r5MiRppubW7U1G4Zh/ulPf7I7VnX+\n",
       "gIAA89VXX7U79vLLL5uGYZi/+tWv7Np/8pOfmIZhmF27djVPnz5ta79165Y5adIk0zAM8+WXX7a1\n",
       "l5SUmJ06dTLd3d3NgwcP2p3rF7/4hWkYhjl27Nhq5+jZs6d56tQph+u80+fQ0M/dMAwzMTHR7nP4\n",
       "9NNPTXd3d3PAgAF2/Xfv3m0ahmGGhISYZ8+erXauKn/7299MwzDMoUOHmpcuXbLr9+abb5qGYZhJ\n",
       "SUm2tvnz55uGYZh//etfHc5bXFxsVx8AWIUAAqBNqfrlr7pXVRjIy8szDcMwp06dWu05/vKXv5iG\n",
       "YZi/+93v6jTns88+axqGYX7xxRcOtTRHAJk4caJD/5s3b5qdOnUyg4KCzJs3bzocv3jxounm5lbj\n",
       "NX9XbQFkxIgRDv33799vGoZhfu9733P4pbawsNA0DMN86qmn7NqrwsHKlSsdzvfZZ5+Z7dq1s3sP\n",
       "tmzZYhqGYc6cOdOh/40bN8zg4GDTMAy7MFM1x9q1a6u9zjt9DrWp7XP38/MzS0pKHMaMGDHCdHNz\n",
       "M0tLS21tjz76qGkYhvmXv/zljnNOmDDBNAzD/PTTT6s9HhERYXbr1s3271UB5P3336/rZQFAs+N7\n",
       "VwBtjmEYunnzZo3Hs7KyJFUuk3nppZccjn/99deSpPz8fLv2Q4cO6Ve/+pWysrL09ddf69q1a3bH\n",
       "z5w5o549ezay+jsbMmSIQ9vJkyd18eJF9evXTz//+c+rHefl5eVwTQ1RtXTpdoGBgZKkiIgIh6Vb\n",
       "QUFBkqQvv/yy2vONHDnSoe2ee+5Rz549VVhYqMuXL6t9+/bKycmRJNt9G7dr166dRowYoc2bNys3\n",
       "N9fhJvrq3rO6asjn3q9fP/n5+Tmcq1evXjJNUxcvXpSPj48k6e9//7vc3Nz0yCOP3LGWrKwseXh4\n",
       "aNu2bdXe03Pt2jV9/fXXunjxojp27Kjp06dr7dq1mjBhgiZPnqxRo0Zp6NChCgkJqc9bAABNigAC\n",
       "wOV8++23kqQ9e/ZUe8O5VBliSktLbf++fft2TZ48WT4+PhozZoxCQkLk6+srNzc37d27Vx9++KEq\n",
       "Kiosqb979+4ObVXX9I9//KPGAPLda2qogIAAh7aq+whqO3b9+vVqz3f33XdX2969e3edPn1aly5d\n",
       "Uvv27W03WFeFne+qaq/uRuzq3rO6aOjn3qFDh2rPV/Ve3B6Qi4uL1bFjR7t7XWry7bff6ubNm7U+\n",
       "t8QwDF25ckUdO3bU4MGDdeDAAb3yyivKyMjQ5s2bJUnf//73tXz5ck2fPv2OcwJAUyOAAHA5Vb8k\n",
       "r127Vv/+7/9epzFLly6Vl5eXPvroI33/+9+3O3bmzBl9+OGH9aqhaoepmnYhKi4uVqdOnao9Vt3N\n",
       "4VXXNHHiRGVkZNSrlpb21VdfqV+/fg7t586dk2EYtmur+md1u11JUlFRkV2/2zX0OR9N/blXp0OH\n",
       "Drp48aLKy8vl5eVVa9+qa/vmm2/qfP6YmBi99957un79uj766CPt2rVLv/71rzVjxgx17drV7gZ3\n",
       "ALAC2/ACcDkPPPCAJNm2Wq2Lf/7znxowYIDDL6G3bt3SwYMHqx1T21Kwjh07SpJOnz5d7VyXL1+u\n",
       "c22SFBYWpg4dOigrK6vVba26b98+h7bPPvtMX3zxhYKDg9W+fXtJUlRUlCRp7969Dv1v3LihAwcO\n",
       "yDAMW7+6aNeunSTV+Dk15HOvrwceeEC3bt3Srl276tT3woUL+vTTT+s9j4eHhx544AGtWLFCa9eu\n",
       "lST99a9/rfd5AKCxCCAAXE50dLSGDx+uP//5z0pNTa22zyeffGK7F0SqvCfh5MmTtr+yS5UPr3vp\n",
       "pZeUn59f7V/YO3fubHs2xneFhYWpffv2evfdd+3mKSsr03PPPVfva2rXrp2effZZFRUV6bnnnlN5\n",
       "eblDn6Kioia5B6Sp/epXv7ILYrdu3dLPfvYzmaapJ5980tY+YcIEderUSenp6Tpy5IjdOdasWaNT\n",
       "p05p9OjR9boPp3PnzpJk28r4uxryudfXs88+K0lasGCBzp4963D89rakpCRJUmJiol1NVUpLS+2e\n",
       "T3L48OFqfxaqvkWqug8FAKzkkkuwKioqtGzZMm3evFnFxcW67777tHLlSo0ePbrWcfv371dKSory\n",
       "8vL09ddfq3379goPD9f/+3//T+PHj3fof/jwYS1cuFC5ublq3769pk6dqldffbXaffABWOtPf/qT\n",
       "Hn74Yc2ZM0dr167VkCFD1KFDB3355Zc6fvy4Tpw4ob///e/q2rWrpMpf/P7t3/5NkZGRmjhxojw8\n",
       "PHTo0CHl5+frscce03vvvecwx+jRo/XWW2/pRz/6kSIjI+Xh4aGRI0dq+PDhcnd31/PPP6+XX35Z\n",
       "kZGRmjBhgm7cuKHMzEz16NFDQUFB9X4699KlS/Xxxx/rv/7rv/Tee+/poYceUo8ePXT+/Hn94x//\n",
       "0OHDh/Xqq68qLCysTuer7/wNNWzYMEVERGjatGlq3769du/erePHj2vQoEFauHChrZ+vr682btyo\n",
       "KVOmaOTIkZoyZYp69eql7Oxs7dmzR4GBgVq3bl295u7fv7969Oiht956Sx4eHurdu7cMw9ATTzyh\n",
       "3r17N+hzr68xY8boxRdf1MqVKxUWFqYJEyaoZ8+e+uqrr3Tw4EE98MADtqD88MMP67XXXtOSJUvU\n",
       "r18/xcXFKTg4WFeuXFFhYaH279+v4cOHa+fOnZKk5ORk7d27V8OHD1dwcLD8/Px04sQJ7dq1S506\n",
       "ddK8efMaXT8A1FsL7sDVYqZPn256eHiYCxcuNDds2GA++OCDpoeHh8O+8t/1xhtvmI8//rj56quv\n",
       "mhs3bjRTUlLMiIgI0zAMc8uWLXZ9c3NzTS8vLzM6Otpct26d+eKLL5peXl7m+PHjm/PSAJdnGIbD\n",
       "9rE1KSkpMV999VUzOjra9PPzM729vc3vfe975qOPPmpu2LDBbqtU06x8zkJERITp6+trdu3a1Zw4\n",
       "caL5P//zP+ZLL71kurm5mR9++KFd//Pnz5szZsww7777brNdu3amm5ubw3avr732mhkSEmLedddd\n",
       "Zp8+fcxFixaZV69eNYODgx224X3zzTdNNzc3c9OmTbVe1+bNm81Ro0aZnTp1Mu+66y6zZ8+e5vDh\n",
       "w81Vq1bZPVOiNrGxsTVuw1vdlrWff/65aRiG+eSTT1Z7vuq2JK7aIvfzzz83//M//9Ps37+/6eXl\n",
       "Zfbs2dNMSkqqdhtb0zTNY8eOmY8//rjZtWtX2/v2zDPPmEVFRQ59Z8+ebbq5uZmFhYU1XuuxY8fM\n",
       "UaNGmQEBAaabm5vDZ1nfz7227Zdrq2fnzp3mI488Ynbq1Mn09PQ0e/fubU6cONHcu3evQ9+DBw+a\n",
       "U6dONYOCgsy77rrL7NatmxkZGWkuWLDAzM7OtvV7//33zSeffNIcMGCAGRAQYPr6+pr9+/c3n3/+\n",
       "ebvtigHASoZpWvQnLidx9OhRxcTEKCUlRfPnz5dU+Y1IeHi4unXrpkOHDtXrfGVlZfre976n0NBQ\n",
       "u5sR4+LidPz4cRUUFNi2YvzDH/6gxMRE7d69W2PGjGm6iwKAVmj27NlKS0vTqVOn1Lt375YuBwBg\n",
       "EZe7ByQjI0Pu7u52Xzt7enpqzpw5ysrK0pkzZ+p1Pm9vb3Xp0kUeHh62tsuXLyszM1MJCQl2+8A/\n",
       "8cQT8vPz07Zt2xp/IQAAAEAr5HL3gOTm5io0NNThAVGDBw+WJOXl5alHjx61nuPy5cu6du2avvnm\n",
       "G6WlpenkyZNKTk62Hf/kk09048YNh4d1eXh4KCIiQrm5uU10NQAAAEDr4nIBpKioqNqHWFW1VbcD\n",
       "yXdNnTpV77//vqTKmyK3bdtmdxN61c4k1c3TvXv3Jtu6EQBaM8MwmmQXKQBA6+JyS7DKysqqfdps\n",
       "1cOfysrK7niOX/ziF9qzZ4/+8Ic/aMCAAZo+fbrd05SrzlHTPHWZAwDautTUVN28eZP7PwDAxbjc\n",
       "NyDe3t6qqKhwaK/aJ93b2/uO5xg4cKDt/05ISFBUVJR++tOf6uTJk3bnqGke9l0HAACAq3K5ABIY\n",
       "GFjtMquqZVNBQUH1Op+Hh4cee+wxvfbaayouLlaHDh1sS6+qe0hUUVFRjXN888032r17t4KDg+sU\n",
       "hAAAAGCtsrIynTp1SuPGjVOXLl1aupxWyeUCSGRkpPbt26eSkhL5+/vb2queqhsREVHvc1YtqXJz\n",
       "q1zRFh4eLnd3dx07dkyTJ0+29bt27Zry8vI0ffr0as+ze/duJSQk1Ht+AAAAWGvLli2aOXNmS5fR\n",
       "KrlcAJk8ebJSUlK0fv16LViwQFLlUqnU1FTFxMTYdsA6d+6ciouL1bdvX7m7V75N58+fV7du3ezO\n",
       "V1xcrHfeeUf33nuv2rdvL0kKCAjQ6NGjtWXLFi1dutS249bmzZtVWlqqKVOmVFtbcHCwpMof6Lo+\n",
       "qdhKSUlJWr16dUuXUS1nrc1Z65KoraGctTZnrUuitoZy1tqctS6J2hrKWWtz1rry8/OVkJBg+70N\n",
       "9edyAWTIkCGaMmWKlixZovPnzyskJESbNm3S6dOnlZqaauu3ePFihwdkjR8/Xr169dKQIUPUrVs3\n",
       "25ivv/5ab775pt08r7zyih588EGNHDlSiYmJ+vLLL/X6669r3LhxGjt2bLW1VS27CgsLU1RUVPO8\n",
       "AY0QEBDglHVJzlubs9YlUVtDOWttzlqXRG0N5ay1OWtdErU1lLPW5qx1VWG5fMO5XACRpLS0NC1d\n",
       "ulSbN2/WxYsXNXDgQO3YsUPDhg2z9alue8g5c+borbfe0po1a1RcXKzOnTtr2LBhWrJkicP/QCIj\n",
       "I5WZmalFixZp/vz5at++vebOnatVq1ZZco0AAACAM3LJAOLp6ank5GS7hwd+V2pqqt03IpL0zDPP\n",
       "6JlnnqnzPEOHDuWZHwAAAMBtXO45IAAAAABaDgEEdRYfH9/SJdTIWWtz1rokamsoZ63NWeuSqK2h\n",
       "nLU2Z61LoraGctbanLUuNJ5hmqbZ0kWgUk5OjqKjo5Wdne3UN10BAAC4Kn5fazy+AQEAAABgGQII\n",
       "AAAAAMsQQAAAAABYhgACAAAAwDIEEAAAAACWIYAAAAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZ\n",
       "AggAAAAAyxBAAAAAAFiGAAIAAADAMgQQAAAAAJYhgAAAAACwDAEEAAAAgGUIIAAAAAAs497SBcB5\n",
       "padXviSpvFwqLJT69JG8vCrb4uMrXwAAAEBdEUBQo9sDRk6OFB1dGUiiolq2LgAAALReLMECAAAA\n",
       "YBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAAACxDAAEA\n",
       "AABgGZ6EjlYpPb3yJUnl5VJhodSnj+TlVdl2+1PcAQAA4DwIIGiVbg8YOTlSdHRlIImKatm6AAAA\n",
       "UDuWYAEAAACwDAEEAAAAgGUIIAAAAAAsQwABAAAAYBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAA\n",
       "AAAAliGAAAAAALAMAQQAAACAZQggAAAAACxDAAEAAABgGQIIAAAAAMsQQAAAAABYhgACAAAAwDIE\n",
       "EAAAAACWIYAAAAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAAyxBAAAAAAFjGJQNIRUWF\n",
       "Fi1apKCgIPn4+CgmJkaZmZl3HPfBBx/oqaeeUmhoqHx9fRUSEqLExESdO3fOoW9sbKzc3NwcXuPH\n",
       "j2+OSwIAAABaBfeWLqAlzJ49W++8846SkpLUr18/paamKi4uTnv37tXQoUNrHLdo0SIVFxdrypQp\n",
       "6tevn/71r3/pN7/5jXbs2KG8vDzdfffddv179eqlVatW2bUFBQU1yzUBAAAArYHLBZCjR49q69at\n",
       "SklJ0fz58yVJs2bNUnh4uBYuXKhDhw7VOHbNmjUaNmyYXdsjjzyikSNH6je/+Y1efvllu2MBAQGa\n",
       "MWNG018EAAAA0Eq53BKsjIwMubu7a968ebY2T09PzZkzR1lZWTpz5kyNY78bPiRp+PDh6tSpkwoK\n",
       "ChyOmaapmzdv6sqVK01TPAAAANDKuVwAyc3NVWhoqPz8/OzaBw8eLEnKy8ur1/muXLmikpISdenS\n",
       "xeHYyZMn5evrq/bt2yswMFDLli3TjRs3Gl48AAAA0Mq53BKsoqIiBQYGOrRXtZ09e7Ze51uzZo2u\n",
       "X7+uadOm2bX37dtXo0aN0r333qvS0lK9/fbbWrlypU6ePKm33nqr4RcAAAAAtGIuF0DKysrk6enp\n",
       "0O7l5WU7Xlf79+/XihUrNG3aNMXGxtode+ONN+z+febMmXr66ae1YcMGJSUl6f77769/8QAAAEAr\n",
       "53JLsLy9vVVRUeHQXl5ebjteFwUFBXr88cd13333OYSNmixYsEBS5Xa+AAAAgCtyuQASGBhY7TKr\n",
       "oqIiSXXbJveLL77Q2LFj1bFjR+3cuVO+vr51mrtnz56SpAsXLtSjYgAAAKDtcLklWJGRkdq3b59K\n",
       "Skrk7+9vaz9y5IgkKSIiotbx3377rcaOHavr169r7969Ds/+qM1nn30mSeratWut/ZKSkhQQEGDX\n",
       "Fh8fr/j4+DrPBQAAgMZJT09Xenq6XdulS5daqJq2w+UCyOTJk5WSkqL169fblkRVVFQoNTVVMTEx\n",
       "6tGjhyTp3LlzKi4uVt++feXuXvk2lZaWKi4uTkVFRdq7d69CQkKqnaOkpER33XWX3b0mpmlq5cqV\n",
       "MgxD48aNq7XG1atXKyoqqikuFwAAAA1U3R+Ac3JyFB0d3UIVtQ0uF0CGDBmiKVOmaMmSJTp//rxC\n",
       "QkK0adMmnT59WqmpqbZ+ixcvVlpamk6dOqXevXtLqryR/NixY3rqqad04sQJnThxwtbf399fP/7x\n",
       "jyVJ2dnZio+P14wZMxQSEqKysjJt375dhw8f1tNPP33Hb1kAAACAtsrlAogkpaWlaenSpdq8ebMu\n",
       "XryogQMHaseOHXYPGjQMQ4Zh2I37+OOPZRiGNm7cqI0bN9odCw4OtgWQ4OBgjRgxQtu3b9e5c+fk\n",
       "5uamAQMGaN26dUpMTGz+CwSqkZ5e+ZKk8nKpsFDq00f6vw3gFB9f+QIAAGhOhmmaZksXgUpVX+ll\n",
       "Z2c73RKsnBwpOlrKzpacrDSnrs1Z8Z4BANAwzvz7WmvhcrtgAQAAAGg5BBAAAAAAliGAAAAAALCM\n",
       "S96EDjQXbvQGAACoHQEEaEK3B4yqG73T07nRGwAAoApLsAAAAABYhgACAAAAwDIEEAAAAACWIYAA\n",
       "AAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAAyxBAAAAAAFiGAAIAAADAMgQQAAAAAJYh\n",
       "gAAAAACwDAEEAAAAgGUIIAAAAAAsQwABAAAAYBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAA\n",
       "liGAAAAAALAMAQQAAACAZQggAAAAACxDAAEAAABgGQIIAAAAAMsQQAAAAABYhgACAAAAwDIEEAAA\n",
       "AACWIYAAAAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAAyxBAAAAAAFiGAAIAAADAMgQQ\n",
       "AAAAAJYhgAAAAACwDAEEAAAAgGUIIAAAAAAsQwABAAAAYBkCCAAAAADLEEAAAAAAWMa9pQsAgPT0\n",
       "ypcklZdLhYVSnz6Sl1dlW3x85QsAALR+BBAALe72gJGTI0VHVwaSqKiWrQsAADQ9lmABAAAAsAwB\n",
       "BAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAAyxBAAAAAAFiGAAIAAADAMi4ZQCoqKrRo0SIFBQXJ\n",
       "x8dHMTExyszMvOO4Dz74QE899ZRCQ0Pl6+urkJAQJSYm6ty5c9X2P3z4sIYNGyZfX18FBgbq+eef\n",
       "V2lpaVNfDgAAANBquGQAmT17tlavXq1Zs2Zp7dq1ateuneLi4nTo0KFaxy1atEj79+/XpEmT9Otf\n",
       "/1rTp0/Xtm3bFBkZqa+++squb15enkaNGqXy8nKtXr1ac+fO1fr16zVlypTmvDQAAADAqbnck9CP\n",
       "Hj2qrVu3KiUlRfPnz5ckzZo1S+Hh4Vq4cGGtIWTNmjUaNmyYXdsjjzyikSNH6je/+Y1efvllW/sL\n",
       "L7ygzp07a9++ffLz85MkBQcHKzExUXv27NGYMWOa4eoAAAAA5+Zy34BkZGTI3d1d8+bNs7V5enpq\n",
       "zpw5ysrK0pkzZ2oc+93wIUnDhw9Xp06dVFBQYGu7fPmyMjMzlZCQYAsfkvTEE0/Iz89P27Zta6Kr\n",
       "AQAAAFoXlwsgubm5Cg0NtQsGkjR48GBJlUun6uPKlSsqKSlRly5dbG2ffPKJbty4oUGDBtn19fDw\n",
       "UEREhHJzcxtYPQAAANC6uVwAKSoqUmBgoEN7VdvZs2frdb41a9bo+vXrmjZtmt0ct5/zdt27d6/3\n",
       "HAAAAEBb4XIBpKysTJ6eng7tXl5etuN1tX//fq1YsULTpk1TbGys3RySapynPnMAAAAAbYnLBRBv\n",
       "b29VVFQ4tJeXl9uO10VBQYEef/xx3XfffXrjjTcc5pBU4zw+Pj71LRsAAABoE1xuF6zAwMBql0BV\n",
       "LZsKCgq64zm++OILjR07Vh07dtTOnTvl6+vrMMft5/zuPHeaIykpSQEBAXZt8fHxio+Pv2NtAAAA\n",
       "aBrp6elKT0+3a7t06VILVdN2uFwAiYyM1L59+1RSUiJ/f39b+5EjRyRJERERtY7/9ttvNXbsWF2/\n",
       "fl179+7V3Xff7dAnPDxc7u7uOnbsmCZPnmxrv3btmvLy8jR9+vRa51i9erWioqLqc1kAAABoYtX9\n",
       "ATgnJ0fe+8PLAAAgAElEQVTR0dEtVFHb4HJLsCZPnqybN29q/fr1traKigqlpqYqJiZGPXr0kCSd\n",
       "O3dOBQUFunHjhq1faWmp4uLiVFRUpJ07dyokJKTaOQICAjR69Ght2bJFV65csbVv3rxZpaWlPIwQ\n",
       "AAAALsvlvgEZMmSIpkyZoiVLluj8+fMKCQnRpk2bdPr0aaWmptr6LV68WGlpaTp16pR69+4tSZo5\n",
       "c6aOHTump556SidOnNCJEyds/f39/fXjH//Y9u+vvPKKHnzwQY0cOVKJiYn68ssv9frrr2vcuHEa\n",
       "O3asdRcMAAAAOBGXCyCSlJaWpqVLl2rz5s26ePGiBg4cqB07dtg9aNAwDBmGYTfu448/lmEY2rhx\n",
       "ozZu3Gh3LDg42C6AREZGKjMzU4sWLdL8+fPVvn17zZ07V6tWrWreiwMAAACcmEsGEE9PTyUnJys5\n",
       "ObnGPqmpqXbfiEjS559/Xq95hg4dqoMHDzaoRgAAAKAtcrl7QAAAAAC0HAIIAAAAAMsQQAAAAABY\n",
       "hgACAAAAwDIEEAAAAACWIYAAAAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAAyxBAAAAA\n",
       "AFiGAAIAAADAMgQQAAAAAJYhgAAAAACwDAEEAAAAgGUIIAAAAAAsQwABAAAAYBkCCAAAAADLEEAA\n",
       "AAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAAACxDAAEAAABgGQIIAAAAAMsQ\n",
       "QAAAAABYhgACAAAAwDLuLV0AADiz9PTKlySVl0uFhVKfPpKXV2VbfHzlCwAA1A0BBABqcXvAyMmR\n",
       "oqMrA0lUVMvWBQBAa8USLAAAAACWIYAAAAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAA\n",
       "yxBAAAAAAFiGAAIAAADAMgQQAAAAAJYhgAAAAACwDAEEAAAAgGUIIAAAAAAsQwABAAAAYBkCCAAA\n",
       "AADLEEAAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAAACxDAAEAAABgGQII\n",
       "AAAAAMsQQAAAAABYxiUDSEVFhRYtWqSgoCD5+PgoJiZGmZmZdxx37tw5LV68WA899JD8/f3l5uam\n",
       "Dz/8sNq+sbGxcnNzc3iNHz++qS8HAAAAaDXcW7qAljB79my98847SkpKUr9+/ZSamqq4uDjt3btX\n",
       "Q4cOrXFcQUGBkpOTFRoaqvvuu09ZWVkyDKPG/r169dKqVavs2oKCgprsOgAAAIDWxuUCyNGjR7V1\n",
       "61alpKRo/vz5kqRZs2YpPDxcCxcu1KFDh2ocO2jQIF24cEEdOnRQRkaGsrKyap0rICBAM2bMaNL6\n",
       "AQAAgNbM5ZZgZWRkyN3dXfPmzbO1eXp6as6cOcrKytKZM2dqHOvn56cOHTrUeS7TNHXz5k1duXKl\n",
       "UTUDAAAAbYXLBZDc3FyFhobKz8/Prn3w4MGSpLy8vCab6+TJk/L19VX79u0VGBioZcuW6caNG012\n",
       "fgAAAKC1cbklWEVFRQoMDHRor2o7e/Zsk8zTt29fjRo1Svfee69KS0v19ttva+XKlTp58qTeeuut\n",
       "JpkDAAAAaG1cLoCUlZXJ09PTod3Ly8t2vCm88cYbdv8+c+ZMPf3009qwYYOSkpJ0//33N8k8AAAA\n",
       "QGvickuwvL29VVFR4dBeXl5uO95cFixYIEn64IMPmm0OAAAAwJm5XAAJDAysdplVUVGRpObdJrdn\n",
       "z56SpAsXLjTbHAAAAIAzc7klWJGRkdq3b59KSkrk7+9vaz9y5IgkKSIiotnm/uyzzyRJXbt2rbVf\n",
       "UlKSAgIC7Nri4+MVHx/fbLUBAADAXnp6utLT0+3aLl261ELVtB0uF0AmT56slJQUrV+/3rYkqqKi\n",
       "QqmpqYqJiVGPHj0kVT71vLi4WH379pW7e/3eppKSEt11111295qYpqmVK1fKMAyNGzeu1vGrV69W\n",
       "VFRUPa8MAAAATam6PwDn5OQoOjq6hSpqG1wugAwZMkRTpkzRkiVLdP78eYWEhGjTpk06ffq0UlNT\n",
       "bf0WL16stLQ0nTp1Sr1797a1r1y5UpJ04sQJSVJaWpr2798vSXrxxRclSdnZ2YqPj9eMGTMUEhKi\n",
       "srIybd++XYcPH9bTTz/drN+yAAAAAM7M5QKIVBkali5dqs2bN+vixYsaOHCgduzYoWHDhtn6GIYh\n",
       "wzAcxi5btkyGYcg0TRmGoY0bN9r6VwWQ4OBgjRgxQtu3b9e5c+fk5uamAQMGaN26dUpMTLTmIgEA\n",
       "AAAn5JIBxNPTU8nJyUpOTq6xT2pqqt03IlVu3bp1x/MHBwdr69atjaoRAGqTnl75kqTycqmwUOrT\n",
       "R/q/HcUVH1/5AgDA2bhkAAGA1u72gJGTI0VHVwYSbh8DADg7l9uGFwAAAEDLIYAAAAAAsAwBBAAA\n",
       "AIBlCCAAAAAALEMAAQAAAGAZAggAAAAAyxBAAAAAAFiGAAIAAADAMgQQAAAAAJYhgAAAAACwDAEE\n",
       "AAAAgGUIIAAAAAAsQwABAAAAYBkCCAAAAADLuLd0AbDG1atXVVBQ0ODx+fneksKUn58vqaxB5+jf\n",
       "v798fHwaXAMAAABaPwKIiygoKFB0dHQjzhApKUcJCTMl5TboDNnZ2YqKinJoJxwBgJSeXvmSpPJy\n",
       "qbBQ6tNH8vKqbIuPr3wBQGtHAHExWySFNWBcvqSEBo6vGlsTZw5HAGCV2wNGTo4UHV0ZSPh/TQDa\n",
       "GgKIiwmT1Jj/ljV2fG2cMRwBAACgaRFA4DScORwBAACgaRBAAABNinsZAAC1IYAAAJoU9zIAAGrD\n",
       "c0AAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZdgFCwDgMtgiGABaHgEEAOAy2CIY\n",
       "AFoeS7AAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAAACzDNrxALa5evaqC\n",
       "goIGjc3P95YUpvz8fEllDa6hf//+8vHxafB4AAAAZ0IAAWpRUFCg6OjoBo6OlJSjhISZknIbXEN2\n",
       "draieEgBAABoIwggQB1skRRWzzH5khIaOPb28QAAAG0JAQSogzBJDf0OojFjAQAA2hpuQgcAAABg\n",
       "Gb4BAVopbpAHAACtEQEEaKW4QR4AALRGBBCgleMGeQAA0JoQQIBWjhvkAQBAa8JN6AAAAAAsQwAB\n",
       "AAAAYBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAAACxD\n",
       "AAEAAABgGZcMIBUVFVq0aJGCgoLk4+OjmJgYZWZm3nHcuXPntHjxYj300EPy9/eXm5ubPvzwwxr7\n",
       "Hz58WMOGDZOvr68CAwP1/PPPq7S0tCkvBQAAAGhVXDKAzJ49W6tXr9asWbO0du1atWvXTnFxcTp0\n",
       "6FCt4woKCpScnKyioiLdd999kiTDMKrtm5eXp1GjRqm8vFyrV6/W3LlztX79ek2ZMqXJrwcAAABo\n",
       "LdxbugCrHT16VFu3blVKSormz58vSZo1a5bCw8O1cOHCWkPIoEGDdOHCBXXo0EEZGRnKysqqse8L\n",
       "L7ygzp07a9++ffLz85MkBQcHKzExUXv27NGYMWOa9sIAAACAVsDlvgHJyMiQu7u75s2bZ2vz9PTU\n",
       "nDlzlJWVpTNnztQ41s/PTx06dLjjHJcvX1ZmZqYSEhJs4UOSnnjiCfn5+Wnbtm2NuwgAAACglXK5\n",
       "AJKbm6vQ0FC7YCBJgwcPllS5dKqxPvnkE924cUODBg2ya/fw8FBERIRyc3MbPQcAAADQGrlcACkq\n",
       "KlJgYKBDe1Xb2bNnm2SO2895u+7duzfJHAAAAEBr5HIBpKysTJ6eng7tXl5etuNNMYekGudpijkA\n",
       "AACA1sjlAoi3t7cqKioc2svLy23Hm2IOSTXO4+Pj0+g5AAAAgNbI5QJIYGBgtUugqpZNBQUFNckc\n",
       "t5/zu/M0xRwAAABAa+Ry2/BGRkZq3759Kikpkb+/v639yJEjkqSIiIhGzxEeHi53d3cdO3ZMkydP\n",
       "trVfu3ZNeXl5mj59eq3jk5KSFBAQYNcWHx+v+Pj4RtcGAACAuklPT1d6erpd26VLl1qomrbD5QLI\n",
       "5MmTlZKSovXr12vBggWSKpdKpaamKiYmRj169JBU+dTz4uJi9e3bV+7u9XubAgICNHr0aG3ZskVL\n",
       "ly617bi1efNmlZaW3vFhhKtXr1ZUVFQDrg4AAABNpbo/AOfk5Cg6OrqFKmobXC6ADBkyRFOmTNGS\n",
       "JUt0/vx5hYSEaNOmTTp9+rRSU1Nt/RYvXqy0tDSdOnVKvXv3trWvXLlSknTixAlJUlpamvbv3y9J\n",
       "evHFF239XnnlFT344IMaOXKkEhMT9eWXX+r111/XuHHjNHbsWCsuFQAAAHA6LhdApMrQsHTpUm3e\n",
       "vFkXL17UwIEDtWPHDg0bNszWxzAMGYbhMHbZsmUyDEOmacowDG3cuNHW//YAEhkZqczMTC1atEjz\n",
       "589X+/btNXfuXK1atar5LxAAAABwUi4ZQDw9PZWcnKzk5OQa+6Smptp9I1Ll1q1bdZ5n6NChOnjw\n",
       "YINqBAAAANoil9sFCwAAAEDLIYAAAAAAsAwBBAAAAIBlXPIeEADN5+rVqyooKGjw+Px8b0lhys/P\n",
       "l1TWoHP0799fPj4+Da4BAAA0HwIIgCZVUFDQyP3RIyXlKCFhpqTcBp0hOzubZ+kAAOCkCCAAmsUW\n",
       "SWENGJcvKaGB46vGAgAA50UAAdAswiQ15juIxo4HAADOiZvQAQAAAFiGAAIAAADAMgQQAAAAAJYh\n",
       "gAAAAACwDAEEAAAAgGUIIAAAAAAsQwABAAAAYBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAA\n",
       "liGAAAAAALAMAQQAAACAZQggAAAAACxDAAEAAABgGQIIAAAAAMsQQAAAAABYhgACAAAAwDIEEAAA\n",
       "AACWcW/pAgAAgPNKT698SVJ5uVRYKPXpI3l5VbbFx1e+AKCuCCAAAKBGtweMnBwpOroykERFtWxd\n",
       "AFovlmABAAAAsAzfgABwGVevXlVBQUGDx+fne0sKU35+vqSyBp2jf//+8vHxaXANAAC0dgQQAC6j\n",
       "oKBA0dHRjThDpKQcJSTMlJTboDNkZ2crirUrAAAXRgAB4HK2SAprwLh8SQkNHF81FgAAV0cAAeBy\n",
       "wiQ15juIxo4HAMCVcRM6AAAAAMvwDQgAAGiVeEYJ0DoRQAAAQKvEM0qA1oklWAAAAAAsQwABAAAA\n",
       "YBkCCAAAAADLEEAAAAAAWIYAAgAAAMAyBBAAAAAAliGAAAAAALAMAQQAAACAZQggAAAAACxDAAEA\n",
       "AABgGQIIAAAAAMsQQAAAAABYxr2lCwAAAGhr0tMrX5JUXi4VFkp9+kheXpVt8fGVL8AVEUAAAACa\n",
       "2O0BIydHio6uDCRRUS1bF+AMCCAA4ASuXr2qgoKCBo3Nz/eWFKb8/HxJZQ2uoX///vLx8WnweAAA\n",
       "6oIAAgBOoKCgQNHR0Q0cHSkpRwkJMyXlNriG7OxsRVXz51nCEQCgKRFAAMCJbJEUVs8x+ZISGjj2\n",
       "9vE1ceZwBABofQggAOBEwiQ19NfsxoytC2cMRwCA1sclt+GtqKjQokWLFBQUJB8fH8XExCgzM7NO\n",
       "Y4uLizVv3jx17dpVfn5+evjhh5Wb6/hXvdjYWLm5uTm8xo8f39SXAwCWqAo49XmFNWLs7eMBAG2H\n",
       "S34DMnv2bL3zzjtKSkpSv379lJqaqri4OO3du1dDhw6tcdytW7f0wx/+UMePH9fChQvVuXNn/e53\n",
       "v1NsbKyys7PVt29fu/69evXSqlWr7NqCgoKa5ZoAAACA1sDlAsjRo0e1detWpaSkaP78+ZKkWbNm\n",
       "KTw8XAsXLtShQ4dqHJuRkaGsrCxlZGRo4sSJkqSpU6cqNDRUy5cv1x//+Ee7/gEBAZoxY0bzXQwA\n",
       "uLDG3BwvNc0N8twcDwD153IBJCMjQ+7u7po3b56tzdPTU3PmzNELL7ygM2fOqEePHjWO7d69uy18\n",
       "SFKXLl00depUbdmyRdevX5eHh4ftmGmaunnzpsrKyuTn59d8FwUALqhxN8dLTXGDPDfHA0D9uVwA\n",
       "yc3NVWhoqEMgGDx4sCQpLy+vxgCSm5tb7X9oBg8erPXr1+vkyZP6wQ9+YGs/efKkfH19de3aNd19\n",
       "991KTEzUsmXL5O7ucm87ADSbxt7g3pib6wEA9edyvwkXFRUpMDDQob2q7ezZs7WOjY2NrXVsVQDp\n",
       "27evRo0apXvvvVelpaV6++23tXLlSp08eVJvvfVWE1wJAEBq/O5fzbF7GMvDAKBmLhdAysrK5Onp\n",
       "6dDu5eVlO16T8vLyOo9944037PrMnDlTTz/9tDZs2KCkpCTdf//9DaofAOD8WB4GZ5aeXvmSpPJy\n",
       "qbBQ6tNH+r9fZxQfX/kCmovLBRBvb29VVFQ4tJeXl9uON8dYSVqwYIE2bNigDz74gAACAC6A5WFw\n",
       "RrcHjJwcKTq6MpCQV2EVlwsggYGB1S6zKioqklT7NrmNGStJPXv2lCRduHChzvUCAFovZ1weBgAt\n",
       "zeUCSGRkpPbt26eSkhL5+/vb2o8cOSJJioiIqHFsRESEDhw4INM0ZRiG3VhfX1+FhobWOvdnn30m\n",
       "SeratWut/ZKSkhQQEGDXFh8fr3i+DwUAALBMenq60qvWq/2fS5cutVA1bYfLBZDJkycrJSVF69ev\n",
       "14IFCyRVPhk9NTVVMTExth2wzp07p+LiYvXt29e2a9XkyZOVkZGhP//5z5o0aZIk6ZtvvtHbb7+t\n",
       "xx57zLYFb0lJie666y67+0VM09TKlStlGIbGjRtXa42rV69m3S4AAEALq+4PwDk5OY28xwsuF0CG\n",
       "DBmiKVOmaMmSJTp//rxCQkK0adMmnT59WqmpqbZ+ixcvVlpamk6dOqXevXtLqgwgMTExevLJJ/Xp\n",
       "p5/anoRumqZWrFhhG5udna34+HjNmDFDISEhKisr0/bt23X48GE9/fTTtX7LAgAAALRlLhdAJCkt\n",
       "LU1Lly7V5s2bdfHiRQ0cOFA7duzQsGHDbH0Mw7BbZiVJbm5u2rlzp372s59p7dq1Kisr05AhQ5SW\n",
       "lqZ+/frZ+gUHB2vEiBHavn27zp07Jzc3Nw0YMEDr1q1TYmKiZdcJAAAAOBuXDCCenp5KTk5WcnJy\n",
       "jX1SU1PtvhGp0qFDB23YsEEbNmyocWxwcLC2bt3aJLUCAAAAbYlbSxcAAAAAwHUQQAAAAABYhgAC\n",
       "AAAAwDIEEAAAAACWIYAAAAAAsAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZl3wQIQAAAJxLenrlS5LK\n",
       "y6XCQqlPH8nLq7ItPr7yhdaPAAIAAIAWd3vAyMmRoqMrA0lUVMvWhabHEiwAAAAAliGAAAAAALAM\n",
       "AQQAAACAZQggAAAAACxDAAEAAABgGQIIAAAAAMsQQAAAAABYhgACAAAAwDIEEAAAAACWIYAAAAAA\n",
       "sAwBBAAAAIBlCCAAAAAALEMAAQAAAGAZAggAAAAAy7i3dAEAAMA6V69eVUFBQYPG5ud7SwpTfn6+\n",
       "pLIG19C/f3/5+Pg0eDyA1o0AAgCACykoKFB0dHQDR0dKylFCwkxJuQ2uITs7W1FRUQ7thCPANRBA\n",
       "AABwQVskhdVzTL6khAaOvX18TZw5HAFoOgQQAABcUJikhv6a3ZixdeGM4QhA0yGAAAAAp+LM4QhA\n",
       "47ELFgAAAADLEEAAAAAAWIYlWAAAALVozO5cUtPs0MXuXGhLCCAAAAC1aNzuXFJT7NDF7lxoSwgg\n",
       "AAAAddDYHbYas7sX0JYQQAAAAOqgsTtssUMXUImb0AEAAABYhgACAAAAwDIswQIAAGil2KELrREB\n",
       "BAAAoJVihy60RgQQAACAVo4dutCaEEAAAABaOXboQmvCTegAAAAALEMAAQAAAGAZAggAAAAAy3AP\n",
       "CAAAAJpcY7YIbortgSW2CHZWBBAAAAA0ucZtEdz47YEltgh2VgQQAAAANJvGbPHb2O2F4ZwIIAAA\n",
       "AGg2jdnil+2B2yZuQgcAAABgGQIIAAAAAMsQQAAAAABYhgACAAAAwDIEEAAAAACWIYAAAAAAsIxL\n",
       "BpCKigotWrRIQUFB8vHxUUxMjDIzM+s0tri4WPPmzVPXrl3l5+enhx9+WLm51T8g5/Dhwxo2bJh8\n",
       "fX0VGBio559/XqWlpU15KQAAAECr4pIBZPbs2Vq9erVmzZqltWvXql27doqLi9OhQ4dqHXfr1i39\n",
       "8Ic/VHp6up577jklJyfr/Pnzio2N1T//+U+7vnl5eRo1apTKy8u1evVqzZ07V+vXr9eUKVOa89IA\n",
       "AAAAp+ZyDyI8evSotm7dqpSUFM2fP1+SNGvWLIWHh2vhwoW1hpCMjAxlZWUpIyNDEydOlCRNnTpV\n",
       "oaGhWr58uf74xz/a+r7wwgvq3Lmz9u3bJz8/P0lScHCwEhMTtWfPHo0ZM6YZrxIAAABwTi73DUhG\n",
       "Robc3d01b948W5unp6fmzJmjrKwsnTlzptax3bt3t4UPSerSpYumTp2qd999V9evX5ckXb58WZmZ\n",
       "mUpISLCFD0l64okn5Ofnp23btjXDlQEAAADOz+UCSG5urkJDQ+2CgSQNHjxYUuXSqdrGRkVFObQP\n",
       "HjxYV69e1cmTJyVJn3zyiW7cuKFBgwbZ9fPw8FBERESN94wAAAAAbZ3LBZCioiIFBgY6tFe1nT17\n",
       "ttFji4qK7Npv171791rnAAAAANoylwsgZWVl8vT0dGj38vKyHa9JeXl5ncZW/bOmvrXNAQAAALRl\n",
       "LhdAvL29VVFR4dBeXl5uO97YsVX/rKmvj49P/QsHAAAA2gCX2wUrMDCw2iVQVcumgoKCGj22aulV\n",
       "Vft3+9Y2R3PLb+S4hoyv65i2Vltj6qrPOGetra19nrePc7ba+Flr3Lmd7fO8fZyz1cbPWuPO7Wyf\n",
       "5+3jnK02q37W0DJcLoBERkZq3759Kikpkb+/v639yJEjkqSIiIgax0ZEROjAgQMyTVOGYdiN9fX1\n",
       "VWhoqCQpPDxc7u7uOnbsmCZPnmzrd+3aNeXl5Wn69Om11piUlKSAgAC7tvj4eMXHx9f9QmuQ0MLj\n",
       "m/Pczlpbc9bV2PM763vWFOOb89zOWhs/ay0zvjnP7ay18bPWMuOb89zOWltz/6zdSXp6utLT0+3a\n",
       "Ll261ELVtB2GaZpmSxdhpaNHjyomJka//OUvtWDBAkmVS6XCw8PVtWtXHT58WJJ07tw5FRcXq2/f\n",
       "vnJ3r8xp27Zt0/Tp0/X2229r0qRJkqRvvvlG/fr10/jx4/WnP/3JNk9cXJw+/vhj/e///q9tx60/\n",
       "/OEPSkxM1K5duzR27FiH2nJychQdHa3s7Oxqd9tqjKtXr6qgoKDB4/PzvZWQEKYtW/IVFtawe1j6\n",
       "9+9f7fKztlpbU9TVGmtrq5+nM9fGz1rDOOvn6cy18bPWMM76eTpzbc39s9YYzfn7msswXdDUqVNN\n",
       "Dw8Pc+HChea6devMBx980LzrrrvMAwcO2Pr85Cc/MQ3DMAsLC21tN2/eNB944AHT39/f/PnPf27+\n",
       "9re/NX/wg//f3p2H13TncRz/3CASCSGxRCiRECqlQu1q6RY7HVvbUTutrWjH2g5TS7UdqkOfLsIE\n",
       "RYtQKo9WJ21QU4RYamupVm2xNWiINJH85g9P7jSSkCrn3Mj79TznIb97Tu7n3px77v3e3+93Tqjx\n",
       "8fExhw4dynIfO3fuNB4eHqZu3brmvffeMy+//LLx9PQ0rVu3zjVXfHy8kWTi4+Pv/IP+k+LjjZGu\n",
       "/+tqXDWbq+Yyhmy3y1WzuWouY8h2u1w1m6vmMoZst8tVs7lqLmNc+/NaflHgJqFL0qJFizRy5Eh9\n",
       "+OGHGjFihNLT0xUdHa1mzZo513E4HFmGWUmSm5ub1q1bpx49emj27NkaM2aMypYtq6+++krVqlXL\n",
       "sm5YWJhiYmLk6empF198UfPmzdOAAQMUFRVlyWMEAAAAXFGBmwMiXT897ptvvqk333wz13UiIyMV\n",
       "GRmZrb1kyZKKiIhQRETELe+nadOm2rx585/KCgAAANxLCmQPCAAAAAB7UIAAAAAAsEyBHIIF3C0f\n",
       "fXR9kaSUFCkkRBo3TvLwuN729NPXFwAAgIKKAgS4gygwAAAAbo4CBPkSPQ0AAAD5EwUI8iUKDAAA\n",
       "gPyJSegAAAAALEMBAgAAAMAyFCAAAAAALEMBAgAAAMAyFCAAAAAALEMBAgAAAMAyFCAAAAAALMN1\n",
       "QAAAAGA7LjJccFCAIFccCAAAgFX4XFFwUIAgVxwIAAAAcKdRgAAA7ih6TwFeB8DNUIAABQRvhrAK\n",
       "+xKs4srHNVd+Hbjy84aCgQIEKCB4QwH44HWv4e91e3jeYDcKEABAgcEHLwCwH9cBAQAAAGAZChAA\n",
       "AAAAlqEAAQAAAGAZChAAAAAAlmESOgAAyBVnDgNwp1GAAACAXFFgALjTGIIFAAAAwDIUIAAAAAAs\n",
       "QwECAAAAwDLMAQFgOya5AgBQcFCAALAdBQYAAAUHBQgA3AS9MwAA3FkUIABwExQYAADcWUxCBwAA\n",
       "AGAZChAAAAAAlmEIFgDkQ8xNAQDkVxQgAJAPUWDceygqARQUFCAAALgACgwABQVzQAAAAABYhgIE\n",
       "AAAAgGUoQAAAAABYhgIEAAAAgGUoQAAAAABYhgIEAAAAgGUoQAAAAABYhgIEAAAAgGUoQAAAAABY\n",
       "hgIEAAAAgGUoQAAAAABYhgIEAAAAgGUoQAAAAABYhgIEAAAAgGUoQAAAAABYhgIEAAAAgGUKZAFy\n",
       "8eJFDRo0SGXKlJG3t7ceeeQR7dq1K8/bnzx5Ut27d1epUqXk4+Ojzp0766effsq2XmBgoNzc3LIt\n",
       "gwcPvpMPBwAAAMg3CtsdwGoZGRlq166dvv32W40ZM0Z+fn5699131bJlS8XHx6tq1ao33f7y5ctq\n",
       "1aqVkpKS9PLLL6tw4cKaNWuWWrRood27d8vX19e5rsPhUFhYmF566aUsvyMkJOSuPDYAAADA1RW4\n",
       "AsV6A7wAABVLSURBVCQqKkpbtmxRVFSU/vKXv0iSunfvrpCQEE2aNElLliy56fbvvvuufvjhB23f\n",
       "vl316tWTJLVp00YPPPCAZs6cqWnTpjnXNcaoQoUKeuaZZ+7eAwIAAADykQI3BCsqKkr+/v7O4kOS\n",
       "Spcure7du2vNmjVKS0u75fYNGjRwFh+SVL16dT366KNavnx5tvWNMUpLS9OVK1fu3IOwyUcffWR3\n",
       "hFy5ajZXzSWR7Xa5ajZXzSWR7Xa5ajZXzSWR7Xa5ajZXzYU/r8AVILt27VLdunWztdevX1/Jyck6\n",
       "dOhQrttmZGTo22+/1UMPPZTj9keOHMlWaHz11VcqVqyYihcvripVqmj27Nl//kHYxJUPBK6azVVz\n",
       "SWS7Xa6azVVzSWS7Xa6azVVzSWS7Xa6azVVz4c8rcAVIQkKCypcvn609s+3UqVO5bpuYmKjU1NQ8\n",
       "b//ggw/q1Vdf1apVqzR//nxVqlRJI0eO1Lhx4/7swwAAAADypXw9B8QYo99++y1P63p4eEiSUlJS\n",
       "VLRo0Vxvv3r1aq6/I/O2vG6/Zs2aLOv07dtXbdq00VtvvaXhw4erQoUKecoOAAAA3CvydQ/Ixo0b\n",
       "VaxYsTwtmUOrPD09cyxaUlJSnLfnJvO2291ekkaNGqVr165p48aNeXuQAAAAwD0kX/eA3H///Vqw\n",
       "YEGe1vX395d0fahUTsOsEhISJEkBAQG5/g5fX18VLVrUue4f3V6SKlasKOn6cK4bZfaeHDx48Ka/\n",
       "wy6XLl3Szp077Y6RI1fN5qq5JLLdLlfN5qq5JLLdLlfN5qq5JLLdLlfN5qq5Mj+n3WzUDG7BFDDd\n",
       "unUz/v7+JiMjI0v7wIEDjbe3t0lNTb3p9vXr1zcNGjTI1v7444+bqlWr3vL+165daxwOh/n444+z\n",
       "3bZ48WIjiYWFhYWFhYWFxcWXxYsX3/JzH3KWr3tAbkfXrl0VFRWlVatWqUuXLpKk8+fPa8WKFerQ\n",
       "oYOKFCniXPfYsWNKTk5WjRo1smw/btw4xcfHO0/F+/333ys2NlajR492rnfhwgWVKFFChQoVcral\n",
       "paXp9ddfV9GiRdWqVats2cLDw7V48WIFBgbecigXAAAArHf16lUdPXpU4eHhdkfJtxzGGGN3CCtl\n",
       "ZGSoWbNm2rdvn0aPHu28EvqJEye0fft2VatWzbluy5YttWnTJmVkZDjbLl++rLCwMCUlJelvf/ub\n",
       "ChcurLfeekvGGO3evVt+fn6SpAULFmjq1Knq1q2bAgMDlZiYqKVLl2r//v2aPn26xo4da/ljBwAA\n",
       "AOxW4HpA3NzctG7dOo0ePVqzZ8/W1atX1aBBAy1atChL8SFJDodDDocjS5u3t7c2bNigUaNGaerU\n",
       "qcrIyFCrVq00a9YsZ/EhSbVr11ZoaKgWL16sc+fOyd3dXWFhYVqxYoWz5wUAAAAoaApcDwgAAAAA\n",
       "++Tr0/ACAAAAyF8oQHBLO3fuVMeOHeXn5ycvLy/VqlVLc+bMsTXTlStXNGnSJLVu3Vq+vr5yc3PT\n",
       "woULXTJDRESEWrRoIX9/f3l4eKhy5cp6+umndeDAAUvz9unTR25ubrkuOZ1e2ko7duxQp06dFBAQ\n",
       "IC8vL91///2aMmWK7ac5/O233zR27FgFBASoWLFiatSokWJiYizNkJd9zRijBQsWqGPHjqpUqZK8\n",
       "vb1Vq1YtTZs2Lc8XbL1TNmzYkOt+FhcXZ2mWG23fvl3Dhg1TaGiovL29VblyZfXo0UOHDx+2LMPt\n",
       "HL/S0tJUs2ZNubm5aebMmRYlzd20adPk5uamWrVq2R1F8fHxat26tXx8fFSiRAmFh4drz549dsfS\n",
       "/v371a1bNwUHB8vLy0t+fn5q0qSJlixZYlmGP7KvHTx4UK1bt1bx4sXl5+enXr166fz587Zni4uL\n",
       "05AhQ1SvXj0VKVJEbm58dL0XFLg5IPhjvvjiC3Xo0EH16tXTxIkT5e3trR9++EEnT560Nde5c+c0\n",
       "ZcoUVa5cWXXq1NGGDRuyzddxlQy7d+9WcHCwOnfurFKlSunHH39URESEoqOjFR8fr5CQEEvyPv/8\n",
       "83riiSeytGVkZOj5559XlSpVVL58eUty5GTv3r1q1qyZAgICNHLkSPn6+uqbb77RpEmTFB8fr9Wr\n",
       "V9uWrU+fPlq5cqVGjRqlatWqKTIyUm3btlVsbKyaNm1qSYa87GtXrlxRv3791LhxYw0ePFhly5Z1\n",
       "PodffvmlvvrqK0uy/t6IESNUv379LG3BwcGW5/i9N954Q1u2bFG3bt1Uu3ZtJSQk6J133lHdunW1\n",
       "detWhYaG3vUMt3P8mjNnjo4fPy5Jlh/rbnTixAm99tpr8vLysj3Lzp071axZM1WuXFn/+Mc/lJ6e\n",
       "rnfffVctWrRQXFycZcfXnBw7dkyXL19Wnz59FBAQoOTkZEVFRenZZ5/V0aNH9fLLL9/1DHnd106c\n",
       "OKHmzZurVKlSmj59upKSkjRjxgzt3btXcXFxWc4QanW2devWaf78+XrwwQcVHBxs6ZcFuIvsPAcw\n",
       "XNulS5dMuXLlTJcuXeyOks1vv/1mzpw5Y4wxZseOHcbhcJiFCxfmmwzx8fHG4XCYiRMn3s2It/T1\n",
       "118bh8Nhpk+fbmuOCRMmGIfDYQ4cOJClvXfv3sbhcJiLFy/akmvbtm3G4XCYmTNnOttSUlJM1apV\n",
       "TZMmTSzLkZd9LTU11WzZsiXbtpMnTzYOh8PExMRYktUYY2JjY43D4TArV6607D7z6ptvvjFpaWlZ\n",
       "2g4fPmw8PDxMz549LcnwR48dZ86cMSVLljRTp07Ntj/aoUePHuaxxx4zLVu2NA888ICtWdq2bWv8\n",
       "/PxMYmKisy0hIcEUL17cJd+70tPTTZ06dUylSpUsub+87muDBw82Xl5e5vjx4862mJgY43A4zNy5\n",
       "c23NdubMGZOSkmKMMWbo0KHG4XDclTywFv1YyNXSpUt19uxZTZs2TdL1b1h/f0piO7m7u6ts2bKS\n",
       "rg89yW8ZKleuLEl35VulP2Lp0qVyOBx65plnbM2Red2bzOczk7+/vwoVKiR3d3c7YikqKkqFCxfW\n",
       "oEGDnG1FixZV//79tWXLFst6AvOyrxUpUkSNGjXK1t65c2dJ0nfffXf3AubCGKOkpCRdu3bN8vvO\n",
       "TePGjVW4cNbO/6pVq6pmzZqWPUd/9Ngxbtw41ahRQ3/961/vdrRb2rRpk1auXKm3335bxhjbe0C+\n",
       "/vprPfbYYypVqpSzzd/fX82bN1d0dLSSk5NtTJedm5ubKlasaNmxP6/72sqVK9W+fXtVrFjR2fbo\n",
       "o48qJCREy5cvtzVb2bJlVbRo0buSAfahAEGuYmJiVKJECR0/flzVq1dX8eLF5ePjoyFDhlg+pvxe\n",
       "8Msvv+js2bPasWOH+vbtq3Llyqlv37625UlLS9Py5cvVtGlTVapUybYcktSvXz+VK1dO/fv31549\n",
       "e3T8+HEtW7ZM77//vl544QXbLsy5a9cuhYSEyNvbO0t75rCi3bt32xHrDzl9+rQkqXTp0pbfd9++\n",
       "feXj4yNPT0898sgjio+PtzxDXhhjdObMGVueo1uJi4vTokWL9Pbbb9sdRenp6Ro+fLgGDhxoyVC1\n",
       "vEhNTc3x+FCsWDGlpqZq7969NqTKKjk5WefPn9eRI0c0a9YsrV+/XmPGjLE7ltPJkyd17tw5PfTQ\n",
       "Q9luq1+/vnbt2mVDKtzrmAOCXB0+fFjXrl1T586dNWDAAL3xxhuKjY3VnDlzdPHiRS1dutTuiPlK\n",
       "hQoVlJqaKkkKCgrSxo0bVaFCBdvyrF+/XomJiS7xrWpAQID++9//qm3btgoLC3O2v/LKK5o8ebJt\n",
       "uRISEnKcG5PZdurUKasj/WFvvvmmfHx81KZNG8vus2jRouratavatm2r0qVLa//+/ZoxY4Yefvhh\n",
       "ffPNN6pTp45lWfJiyZIlOnXqlKZOnWp3lCyMMRo+fLieeuopNWzYUEePHrU1z/vvv69jx47ZMp8o\n",
       "N9WrV9eWLVuUkZHhnJycmpqqbdu2SXKN1+iLL76ouXPnSpIKFy6s2bNnZ+lVtVvmCUhyO9YlJiYq\n",
       "LS3N9h573FsoQJCry5cvKzk5WYMHD3Z++9a5c2elpqbqgw8+0OTJk1W1alWbU+Yf69evV0pKig4c\n",
       "OKCZM2fqiSee0ObNm7N0eVtp6dKlcnd3V/fu3W25/987c+aM8wNyRESE/Pz8FB0drWnTpqlcuXIa\n",
       "OnSoLbmuXr2aY9e/h4eH83ZX9tprr+nLL7/Ue++9pxIlSlh2v40bN1bjxo2dP7dv315du3ZV7dq1\n",
       "NX78eH322WeWZbmV7777TkOHDlWTJk3Uu3dvu+NksWDBAu3bt0+rVq2yO4p++eUXTZw4URMnTsxy\n",
       "0V27DRkyRIMHD1b//v01ZswYpaena+rUqc6eP1d4jY4aNUrdu3fXqVOntGTJEg0bNkyenp4us79l\n",
       "Pke3OtZRgOBOYggWcpXZrf30009nac/8eevWrZZnys9atGih8PBwjRo1Sps2bdLZs2dt+3b/8uXL\n",
       "WrNmjcLDw7OMnbbLlClTdPLkScXGxqp///7q3Lmz5s2bp969e2vs2LFKTEy0JZenp2eOww1TUlKc\n",
       "t7uqZcuW6e9//7sGDBig5557zu44Cg4OVqdOnRQbG2vbvK0bnT59Wu3atVOpUqUUFRVl+3yG3/v1\n",
       "1181fvx4jRkzxtae0kyvvPKKSpcureHDh9sdJYvnnntOEyZM0NKlSxUaGqratWvrp59+cg5xunH4\n",
       "pB2qV6+uRx55RD179tRnn32mRx99VCNHjnSJ4kj6/3Esvx7rkD9RgCBXAQEBkqRy5cplac+cNHbh\n",
       "wgXLM90rgoKCVKdOHduuibB69WpdvXrVJYZfSdLmzZsVFhbm3OcydejQQcnJybbNtShfvnyOQzgy\n",
       "hyzcmNdV/Oc//1GvXr3Uvn17vf/++3bHcapYsaJSU1N15coVu6Po0qVLatOmjX799Vd9/vnn8vf3\n",
       "tztSFjNmzFBaWpq6d++uo0eP6ujRozpx4oQkKTExUUePHlVaWpolWQ4fPqyIiAgNHz5cJ06ccOZJ\n",
       "SUlRamqqfv75Z1vfD6ZOnaozZ85o8+bN2rt3r7Zt26b09HRJsvU0vLnp0qWLLl26pO+//97uKJL+\n",
       "P/Qqp2tBJSQkyM/Pj94P3HEUIMhV5oS0zDe9TJkfyMqUKWN5pnvJ1atXbbug0pIlS1S8eHF17NjR\n",
       "lvu/UVpamvMDw43tkmw7i1JYWJgOHTqkpKSkLO2Z48tdbS6DdD3bk08+qQYNGmj58uUuddGuH3/8\n",
       "UZ6enrZ/K52SkqIOHTrohx9+UHR0tGrUqGFrnpwcP35cFy5cUGhoqIKCghQUFKTmzZtLuj60Ligo\n",
       "SAcPHrQky8mTJ5WRkaEXXnjBmSUoKEhxcXE6dOiQqlSpoilTpliSJTclS5ZUkyZNnJPjY2JidN99\n",
       "97nk3zaz58NVXpsVKlRQmTJltH379my3xcXFueRxDvmfa+z9cEmZcwPmz5+fpX3evHkqUqSIWrZs\n",
       "aUOq/CU9PT3Hbwbj4uK0b98+Pfzww5ZnOnfunGJiYvTkk086x/farW7dutq5c2e2C0x99NFHKlSo\n",
       "kGrXrm1Lrq5duyo9Pd05gVS6PkwhMjJSjRo1comhMb938OBBtWvXTkFBQYqOjrbt1JXnzp3L1rZn\n",
       "zx59+umn2S6GabX09HT16NFD27Zt04oVK9SwYUNb8+TmhRde0OrVq7MsH3zwgaTrZxdbvXq1AgMD\n",
       "LclSq1YtffLJJ1myfPLJJwoNDVXlypW1evVq9e/f35IsebFs2TLt2LFDI0eOtDVHTq+DtLQ0LVq0\n",
       "SH5+fi5zJjHpeq9MdHR0li8cv/zySx0+fFjdunWzMRnuVUxCR67q1Kmjfv366d///reuXbum5s2b\n",
       "a8OGDYqKitKECRNsH7Lwzjvv6OLFi84emU8//VTHjh2TdP3N24pJt7fKkJGRofvuu09PPfWUatas\n",
       "KS8vL+3du1eRkZHy9/fX+PHj73rGGy1btkzp6ekuM/xKkkaPHq2VK1fq4Ycf1rBhw+Tr66vo6Gh9\n",
       "/vnnGjhwoG37WoMGDdStWzeNHz9eZ8+eVXBwsBYuXKhjx44pMjLS0iy32tccDofCw8N18eJFjRkz\n",
       "RmvXrs2yfdWqVXO8Tsjd0KNHDxUrVkyNGzdW2bJldeDAAc2dO1fe3t56/fXXLcmQm5deeklr165V\n",
       "hw4ddP78eS1evDjL7T179rQkx63+nmFhYVnOCCfJeRas0NBQS3sv/fz81KlTp2zts2bNkiRbe1I3\n",
       "bdqkyZMnKzw8XL6+vtq6dasWLFigNm3aaMSIEbblkqRBgwYpKSlJzZs3V0BAgE6fPq0lS5bo0KFD\n",
       "ioyMVKFChSzJkZf3ygkTJmjFihVq1aqVRowYoaSkJP3zn/9U7dq17+rp4vOS7eeff9aHH34oSdqx\n",
       "Y4ckadq0aTLGKDAw0LLXLO4we65/iPwiLS3NvPrqqyYwMNC4u7ubkJAQ869//cvuWMYYYwIDA43D\n",
       "4TAOh8O4ubkZNzc35/9//vlnl8iQmppqRo4caR588EHj4+Nj3N3dTXBwsBk2bJg5ffq0JRlv1Lhx\n",
       "Y+Pv728yMjJsuf/cbNu2zbRu3dqUKFHCuLu7mxo1apjp06eb9PR0W3OlpKSY0aNHm/LlyxsPDw/T\n",
       "sGFD88UXX1ie41b72k8//eT8OXO93y99+/a1LOvs2bNNw4YNjZ+fnylSpIipUKGC6dWrlzly5Ihl\n",
       "GXLTsmXLXJ8jNzc3y3LczvEr829s95XQM7Vs2dLUqlXL1gxHjhwx4eHhpkyZMsbDw8PUrFnTvPHG\n",
       "G9mudm+Hjz/+2Dz++OPG39/fFClSxPj5+Zm2bduamJgYS3PkdV/bv3+/CQ8PN15eXsbX19c8++yz\n",
       "5uzZs7Zni42NzXEdh8NhWrVqdVfz4e5xGOMipyMBAAAAcM9jDggAAAAAy1CAAAAAALAMBQgAAAAA\n",
       "y1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgA\n",
       "AAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAM\n",
       "BQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAA\n",
       "ALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CA\n",
       "AAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy1CAAAAAALAMBQgAAAAAy/wPAUXa8tM4qjoAAAAA\n",
       "SUVORK5CYII=\n"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This feature selection plots forest importances\n",
    "#http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=10,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False)\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "# Comment these out to use the base example\n",
    "X = np.array(allTrain)\n",
    "y = clsTrain\n",
    "\n",
    "forest.fit(X, y)\n",
    "\n",
    "importances = forest.feature_importances_ #array with importances of each feature\n",
    "\n",
    "idx = np.arange(0, X.shape[1]) #create an index array, with the number of features\n",
    "\n",
    "features_to_keep = idx[importances > np.mean(importances)] #only keep features whose importance is greater than the mean importance\n",
    "#should be about an array of size 3 (about)\n",
    "# print features_to_keep.shape\n",
    "\n",
    "x_feature_selected = X[:,features_to_keep] #pull X values corresponding to the most important features\n",
    "\n",
    "# print x_feature_selected\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f],\":\"+ Labels[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "# interesting enough this feature selection method doesn't fix the same fields as the RFE model\n",
    "from IPython.display import Image\n",
    "Image(\"forest_feature_selection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now I'm taking the advice of my forest of trees field selection\n",
    "\n",
    "def reduceArray(input_array):\n",
    "    output_array = []\n",
    "    for i in range(len(input_array)):\n",
    "        temp = []\n",
    "        temp.append(input_array[i][6]) # gravity_x\n",
    "#         temp.append(input_array[i][1]) # attitude_roll\n",
    "        temp.append(input_array[i][13]) # mag_field_y\n",
    "        output_array.append(temp)\n",
    "    \n",
    "    return output_array\n",
    "    \n",
    "trainGM = reduceArray(allTrain)\n",
    "testGM = reduceArray(allTest)\n",
    "walkGM = reduceArray(walkTest)\n",
    "sitGM = reduceArray(sitTest)\n",
    "carGM = reduceArray(carTest)\n",
    "jogGM = reduceArray(jogTest)\n",
    "stairsGM = reduceArray(stairsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for all test data: 1.0\n",
      "Walking: Score for walk test  1.0 :Results  [0 0 0 0 0 0]\n",
      "Sitting: Score for sit test  1.0 :Results  [1 1 1 1 1 1]\n",
      "Car drive: Score for car test  1.0 :Results  [2 2 2 2 2 2]\n",
      "Jogging: Score for jog test  1.0 :Results  [3 3 3 3 3 3]\n",
      "Stairs: Score for stairs test  1.0 :Results  [4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "#SVM on reduced elements\n",
    "\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(trainGM, clsTrain)  \n",
    "\n",
    "svcwalkResults = svc.predict(walkGM)\n",
    "svcsitResults = svc.predict(sitGM)\n",
    "svccarResults =  svc.predict(carGM)\n",
    "svcjogResults =  svc.predict(jogGM)\n",
    "svcstairsResults =  svc.predict(stairsGM)\n",
    "\n",
    "print \"Score for all test data:\",svc.score(testGM, clsTest)\n",
    "# In multi-label classification, this is the subset accuracy which is a harsh metric since you require for \n",
    "# each sample that each label set be correctly predicted.\n",
    "\n",
    "\n",
    "# print \"Column Data Types\"\n",
    "# print dataType.values()\n",
    "print \"Walking: Score for walk test \",svc.score(walkGM, [0,0,0,0,0,0]),\":Results \", svcwalkResults\n",
    "print \"Sitting: Score for sit test \",svc.score(sitGM, [1,1,1,1,1,1]),\":Results \", svcsitResults\n",
    "print \"Car drive: Score for car test \",svc.score(carGM, [2,2,2,2,2,2]),\":Results \", svccarResults\n",
    "print \"Jogging: Score for jog test \",svc.score(jogGM, [3,3,3,3,3,3]),\":Results \", svcjogResults\n",
    "print \"Stairs: Score for stairs test \",svc.score(stairsGM, [4,4,4,4,4,4]),\":Results \", svcstairsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, so I'm able to achieve 100% accuracy by using the mean gravity_x and mag_field_y only...  I never would have found that combination on my own.  I wonder how it does using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for all test data: 0.966666666667\n",
      "Column Data Types\n",
      "['Walking', 'Sitting', 'Car', 'Jogging', 'Stairs']\n",
      "Walking: Score for walk test  0.833333333333 \n",
      "[[0.712 0.000 0.000 0.288 0.000]\n",
      " [0.727 0.000 0.000 0.273 0.000]\n",
      " [0.780 0.000 0.000 0.220 0.000]\n",
      " [0.972 0.000 0.000 0.028 0.000]\n",
      " [0.991 0.000 0.000 0.009 0.000]\n",
      " [0.365 0.000 0.000 0.635 0.000]]\n",
      "Sitting: Score for sit test  1.0 \n",
      "[[0.019 0.981 0.000 0.000 0.000]\n",
      " [0.019 0.981 0.000 0.000 0.000]\n",
      " [0.019 0.981 0.000 0.000 0.000]\n",
      " [0.018 0.982 0.000 0.000 0.000]\n",
      " [0.018 0.982 0.000 0.000 0.000]\n",
      " [0.018 0.982 0.000 0.000 0.000]]\n",
      "Car drive: Score for car test  1.0 \n",
      "[[0.009 0.000 0.986 0.005 0.000]\n",
      " [0.011 0.000 0.984 0.005 0.000]\n",
      " [0.010 0.000 0.983 0.006 0.000]\n",
      " [0.010 0.000 0.987 0.003 0.000]\n",
      " [0.009 0.000 0.987 0.004 0.000]\n",
      " [0.007 0.000 0.988 0.005 0.000]]\n",
      "Jogging: Score for jog test  1.0 \n",
      "[[0.437 0.000 0.000 0.563 0.000]\n",
      " [0.383 0.000 0.000 0.617 0.000]\n",
      " [0.430 0.000 0.000 0.570 0.000]\n",
      " [0.359 0.000 0.000 0.641 0.000]\n",
      " [0.433 0.000 0.000 0.567 0.000]\n",
      " [0.320 0.000 0.000 0.680 0.000]]\n",
      "Stairs: Score for stairs test  1.0 \n",
      "[[0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]\n",
      " [0.000 0.000 0.000 0.000 1.000]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression ML\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "logistic.fit(trainGM, clsTrain)\n",
    "\n",
    "logwalkResults = logistic.predict_proba(walkGM)\n",
    "logsitResults = logistic.predict_proba(sitGM)\n",
    "logcarResults =  logistic.predict_proba(carGM)\n",
    "logjogResults =  logistic.predict_proba(jogGM)\n",
    "logstairsResults =  logistic.predict_proba(stairsGM)\n",
    "\n",
    "print \"Score for all test data:\",logistic.score(testGM, clsTest)\n",
    "# In multi-label classification, this is the subset accuracy which is a harsh metric since you require for \n",
    "# each sample that each label set be correctly predicted.\n",
    "\n",
    "print \"Column Data Types\"\n",
    "print dataType.values()\n",
    "print \"Walking: Score for walk test \",logistic.score(walkGM, [0,0,0,0,0,0]),\"\\n\", logwalkResults\n",
    "print \"Sitting: Score for sit test \",logistic.score(sitGM, [1,1,1,1,1,1]),\"\\n\", logsitResults\n",
    "print \"Car drive: Score for car test \",logistic.score(carGM, [2,2,2,2,2,2]),\"\\n\", logcarResults\n",
    "print \"Jogging: Score for jog test \",logistic.score(jogGM, [3,3,3,3,3,3]),\"\\n\", logjogResults\n",
    "print \"Stairs: Score for stairs test \",logistic.score(stairsGM, [4,4,4,4,4,4]),\"\\n\", logstairsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So interestingly only those two fields are also really accurate using Logistic Regression, except for Jogging and walking.  For those two it is less than 100% certain.\n",
    "\n",
    "One final comment.  When I used my walking file, which was the only file different from what I'm doing currently, I would get different results from the feature selection, and from some of the ML algorithms.  It seems to me that to generalize from one person's data out to a generic persons would be a very difficult task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

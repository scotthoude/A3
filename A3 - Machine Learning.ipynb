{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading all of the data files, all at 100mhz or 100 samples per second\n",
    "# Thanks to Katie for her files, I needed the sitting, jogging and stairs ones,\n",
    "# the others are my own\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "cls = [] # this is store my array of classifiers (0-4) - kinda hacky but it works\n",
    "\n",
    "#This takes in the csv files and returns an array that can be used for machine learning\n",
    "#also returns the header row which is the same for all of my files\n",
    "def load_files(input_file, cls_num):\n",
    "    output_array = []\n",
    "    input_headers = []\n",
    "    with open(input_file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        input_headers = reader.next() #saves the headers\n",
    "        for row in reader:\n",
    "            output_array.append(row)\n",
    "            \n",
    "    # Prior testing gave me the size of the files so I know I can skip the first 10 seconds (1000 rows)\n",
    "    # and then take the next 30,000 records to get 30 - 10 second chunks\n",
    "    # never forget that it's all zero indexed\n",
    "    output_array = output_array[999:30999]\n",
    "    \n",
    "    # Now let's reduce down to 10 second chunks aka 1000 rows\n",
    "    compressed = []\n",
    "    n = 0\n",
    "    temp = []\n",
    "    cls_list =[]\n",
    "    \n",
    "    for row in output_array:\n",
    "        temp.append(row)\n",
    "        n +=1\n",
    "        if n == 1000: \n",
    "            temp = np.array(temp).astype(float) #convert to a numpy float array so I can do the next line\n",
    "            compressed.append(temp.mean(axis=0).tolist()) #this averages by columns and makes it a list of lists\n",
    "            temp = []\n",
    "            cls_list.append(cls_num)\n",
    "            n = 0\n",
    "            \n",
    "    cls.append(cls_list)\n",
    "#     compressed = list(itertools.chain.from_iterable(compressed)) # this flatens the list - which turns to not be wanted\n",
    "\n",
    "    return compressed, input_headers\n",
    "    \n",
    "rawWalkValues, Labels = load_files(\"walking2.csv\",0)\n",
    "rawSittingValues, Labels = load_files(\"sitting.csv\",1)\n",
    "rawCarValues, Labels = load_files(\"car.csv\",2)\n",
    "rawJogValues, Labels = load_files(\"Jogging.csv\",3)\n",
    "rawStairsValues, Labels = load_files(\"Steps.csv\",4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This block is just to test my data loads and various setup stuff from the block above\n",
    "# print Labels\n",
    "# print rawWalkValues[0]\n",
    "# print cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing the length of the arrays, I should have 30 records after chunking into 10 second sections by / 1000\n",
    "# print \"walk -\",len(rawWalkValues)\n",
    "# print \"sit -\",len(rawSittingValues)\n",
    "# print \"car -\",len(rawCarValues)\n",
    "# print \"jog -\",len(rawJogValues)\n",
    "# print \"stairs -\",len(rawStairsValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.11069621e+04  -3.54022814e-01  -1.13514583e+00  -1.80799684e+00\n",
      "   8.89886132e-01   1.68731766e-01  -1.16298015e+01   1.50421219e+01\n",
      "   2.01558452e+01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# # convert to right data shape, list of lists\n",
    "# X = [[] for k in range(len(rawLabels))]\n",
    "# for row in rawWalkValues:\n",
    "#     for col in range(len(rawLabels)):\n",
    "#         X[col].append(row[col])\n",
    "        \n",
    "sel = VarianceThreshold(threshold=(.8*(1-.8)))\n",
    "print sel.fit_transform(rawWalkValues)[0]\n",
    "\n",
    "# So this strips out the low variance stuff but doesn't help me identify the right features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2\n",
    "# iris = load_iris()\n",
    "# # X, y = rawWalkValues, rawLabels\n",
    "\n",
    "# X = [[] for k in range(len(rawLabels))]\n",
    "# for row in rawWalkValues:\n",
    "#     for col in range(len(rawLabels)):\n",
    "#         X[col].append(row[col])\n",
    "\n",
    "# X = np.array(X)\n",
    "# X.shape\n",
    "\n",
    "# X_new = SelectKBest(chi2, k=2).fit_transform(X, rawLabels)\n",
    "# X_new.shape\n",
    "\n",
    "# This doesn't work for negative values which is what I have - so this selection is out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So this doesn't work either since I don't have the correct data sets yet\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator,3,step=1) # select the three most informative features\n",
    "\n",
    "# convert to right data shape, list of lists\n",
    "X = [[] for k in range(len(rawLabels))]\n",
    "for row in rawWalkValues:\n",
    "    for col in range(len(rawLabels)):\n",
    "        X[col].append(row[col])\n",
    "\n",
    "y = []\n",
    "for i in range(16):\n",
    "    y.append(i)\n",
    "    \n",
    "# selector = selector.fit(X[1:], rawLabels[1:])\n",
    "\n",
    "# selector = selector.fit(X, y)\n",
    "\n",
    "# print(RFE.support_)\n",
    "# print(RFE.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1193, 1456, 1631, ...,  857,  878,  931])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selector.support_\n",
    "selector.ranking_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3L,)\n",
      "1. feature 0 (0.250398)\n",
      "2. feature 1 (0.232397)\n",
      "3. feature 2 (0.148898)\n",
      "4. feature 3 (0.055363)\n",
      "5. feature 8 (0.054010)\n",
      "6. feature 5 (0.053878)\n",
      "7. feature 6 (0.052583)\n",
      "8. feature 9 (0.051020)\n",
      "9. feature 7 (0.050963)\n",
      "10. feature 4 (0.050489)\n"
     ]
    }
   ],
   "source": [
    "#Let's do some feature selection - which isn't the same as extraction\n",
    "#http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Build a classification task using 3 informative features\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=10,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False)\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)\n",
    "\n",
    "# This doesn't work since my X and y are not right at this point\n",
    "\n",
    "# convert to right data shape, list of lists\n",
    "# X = [[] for k in range(len(rawLabels))]\n",
    "# for row in rawWalkValues:\n",
    "#     for col in range(len(rawLabels)):\n",
    "#         X[col].append(row[col])\n",
    "                     \n",
    "# # X = np.array(X)\n",
    "# forest.fit(X, rawLabels)\n",
    "\n",
    "importances = forest.feature_importances_ #array with importances of each feature\n",
    "\n",
    "idx = np.arange(0, X.shape[1]) #create an index array, with the number of features\n",
    "\n",
    "features_to_keep = idx[importances > np.mean(importances)] #only keep features whose importance is greater than the mean importance\n",
    "#should be about an array of size 3 (about)\n",
    "print features_to_keep.shape\n",
    "\n",
    "x_feature_selected = X[:,features_to_keep] #pull X values corresponding to the most important features\n",
    "\n",
    "# print x_feature_selected\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.48006984 -1.35738004  0.46249366  0.74380015  0.59214909 -0.78646842\n",
      "  -1.17647315 -1.28080671  1.66165186 -0.06794512]\n",
      " [-0.55838227  0.24923089 -0.19986189  2.36022859  0.5555456   0.43952232\n",
      "   0.30627249  0.99914985 -0.96606319  2.16001311]]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print X[0:2]\n",
    "print y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn the raw data into a more compressed format and split into training and testing arrays\n",
    "# based on all my files being a little longer than needed I'm going to drop the first 10 seconds (1000 rows)\n",
    "# take the next 24 (10 second chunks or 24000 rows) as training, and then the next 6 (6000 rows) as testing\n",
    "compressed = []\n",
    "\n",
    "n = 0\n",
    "temp = 0\n",
    "\n",
    "for row in rawWalkValues:\n",
    "    temp += float(row[7]) # TODO:  What column should I use? and would it be different for different files\n",
    "    n +=1\n",
    "    if n == 1000:\n",
    "        compressed.append(temp/n)\n",
    "        temp = 0\n",
    "        n = 0\n",
    "    \n",
    "#splitting it up into two sets\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "i = 0\n",
    "for i in range(len(compressed)):\n",
    "    if i < 24:\n",
    "        train.append(compressed[1])\n",
    "    elif i < 30:\n",
    "        test.append(compressed[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987]\n",
      "[-0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987, -0.18755640999999987]\n"
     ]
    }
   ],
   "source": [
    "print train\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where the Machine Learning happens\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "  \n",
    "\n",
    "    \n",
    "# >>> from sklearn import tree\n",
    "# >>> X = [[0, 0], [1, 1]] training set\n",
    "# >>> Y = [0, 1] training labels\n",
    "# >>> clf = tree.DecisionTreeClassifier()\n",
    "# >>> clf = clf.fit(X, Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
